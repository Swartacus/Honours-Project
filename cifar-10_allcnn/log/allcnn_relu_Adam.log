I1011 20:26:22.329609  4939 caffe.cpp:217] Using GPUs 0
I1011 20:26:22.332377  4939 caffe.cpp:222] GPU 0: GeForce GTX 750 Ti
I1011 20:26:22.439313  4939 solver.cpp:48] Initializing solver from parameters: 
train_net: "/home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_train.prototxt"
test_net: "/home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_test.prototxt"
test_iter: 100
test_interval: 200
base_lr: 0.0001
display: 50
max_iter: 2500
lr_policy: "step"
gamma: 0.1
weight_decay: 0.0005
stepsize: 5000
snapshot: 5000
snapshot_prefix: "cifar-10_relu_Adam"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
type: "Adam"
I1011 20:26:22.439476  4939 solver.cpp:81] Creating training net from train_net file: /home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_train.prototxt
I1011 20:26:22.439784  4939 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "../../../data/cifar-10/cifar10_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "bn1"
  top: "conv3"
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "conv3"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "drop1"
  top: "drop1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "drop1"
  top: "conv4"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv5"
  top: "bn2"
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "bn2"
  top: "conv6"
  convolution_param {
    num_output: 192
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "conv6"
  top: "drop4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "drop4"
  top: "drop4"
}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "drop4"
  top: "conv7"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "conv7"
  top: "conv8"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "conv8"
  top: "conv9"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}
layer {
  name: "pool"
  type: "Pooling"
  bottom: "conv9"
  top: "pool"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "flatten"
  type: "Flatten"
  bottom: "pool"
  top: "flatten"
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "flatten"
  top: "score"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
I1011 20:26:22.439923  4939 layer_factory.hpp:77] Creating layer data
I1011 20:26:22.440343  4939 net.cpp:100] Creating Layer data
I1011 20:26:22.440366  4939 net.cpp:408] data -> data
I1011 20:26:22.440382  4939 net.cpp:408] data -> label
I1011 20:26:22.440408  4939 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I1011 20:26:22.451647  4944 db_lmdb.cpp:35] Opened lmdb ../../../data/cifar-10/cifar10_train_lmdb
I1011 20:26:22.451855  4939 data_layer.cpp:41] output data size: 64,3,32,32
I1011 20:26:22.453874  4939 net.cpp:150] Setting up data
I1011 20:26:22.453912  4939 net.cpp:157] Top shape: 64 3 32 32 (196608)
I1011 20:26:22.453915  4939 net.cpp:157] Top shape: 64 (64)
I1011 20:26:22.453917  4939 net.cpp:165] Memory required for data: 786688
I1011 20:26:22.453938  4939 layer_factory.hpp:77] Creating layer label_data_1_split
I1011 20:26:22.453948  4939 net.cpp:100] Creating Layer label_data_1_split
I1011 20:26:22.453953  4939 net.cpp:434] label_data_1_split <- label
I1011 20:26:22.453964  4939 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1011 20:26:22.453972  4939 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1011 20:26:22.454082  4939 net.cpp:150] Setting up label_data_1_split
I1011 20:26:22.454100  4939 net.cpp:157] Top shape: 64 (64)
I1011 20:26:22.454103  4939 net.cpp:157] Top shape: 64 (64)
I1011 20:26:22.454107  4939 net.cpp:165] Memory required for data: 787200
I1011 20:26:22.454108  4939 layer_factory.hpp:77] Creating layer conv1
I1011 20:26:22.454135  4939 net.cpp:100] Creating Layer conv1
I1011 20:26:22.454139  4939 net.cpp:434] conv1 <- data
I1011 20:26:22.454144  4939 net.cpp:408] conv1 -> conv1
I1011 20:26:22.589542  4939 net.cpp:150] Setting up conv1
I1011 20:26:22.589567  4939 net.cpp:157] Top shape: 64 96 30 30 (5529600)
I1011 20:26:22.589570  4939 net.cpp:165] Memory required for data: 22905600
I1011 20:26:22.589588  4939 layer_factory.hpp:77] Creating layer conv2
I1011 20:26:22.589601  4939 net.cpp:100] Creating Layer conv2
I1011 20:26:22.589604  4939 net.cpp:434] conv2 <- conv1
I1011 20:26:22.589608  4939 net.cpp:408] conv2 -> conv2
I1011 20:26:22.590914  4939 net.cpp:150] Setting up conv2
I1011 20:26:22.590924  4939 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1011 20:26:22.590926  4939 net.cpp:165] Memory required for data: 42173184
I1011 20:26:22.590932  4939 layer_factory.hpp:77] Creating layer bn1
I1011 20:26:22.590939  4939 net.cpp:100] Creating Layer bn1
I1011 20:26:22.590941  4939 net.cpp:434] bn1 <- conv2
I1011 20:26:22.590945  4939 net.cpp:408] bn1 -> bn1
I1011 20:26:22.591141  4939 net.cpp:150] Setting up bn1
I1011 20:26:22.591146  4939 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1011 20:26:22.591148  4939 net.cpp:165] Memory required for data: 61440768
I1011 20:26:22.591156  4939 layer_factory.hpp:77] Creating layer conv3
I1011 20:26:22.591161  4939 net.cpp:100] Creating Layer conv3
I1011 20:26:22.591163  4939 net.cpp:434] conv3 <- bn1
I1011 20:26:22.591166  4939 net.cpp:408] conv3 -> conv3
I1011 20:26:22.592322  4939 net.cpp:150] Setting up conv3
I1011 20:26:22.592331  4939 net.cpp:157] Top shape: 64 96 13 13 (1038336)
I1011 20:26:22.592334  4939 net.cpp:165] Memory required for data: 65594112
I1011 20:26:22.592340  4939 layer_factory.hpp:77] Creating layer drop1
I1011 20:26:22.592345  4939 net.cpp:100] Creating Layer drop1
I1011 20:26:22.592347  4939 net.cpp:434] drop1 <- conv3
I1011 20:26:22.592350  4939 net.cpp:408] drop1 -> drop1
I1011 20:26:22.592401  4939 net.cpp:150] Setting up drop1
I1011 20:26:22.592419  4939 net.cpp:157] Top shape: 64 96 13 13 (1038336)
I1011 20:26:22.592422  4939 net.cpp:165] Memory required for data: 69747456
I1011 20:26:22.592423  4939 layer_factory.hpp:77] Creating layer relu3
I1011 20:26:22.592452  4939 net.cpp:100] Creating Layer relu3
I1011 20:26:22.592453  4939 net.cpp:434] relu3 <- drop1
I1011 20:26:22.592456  4939 net.cpp:395] relu3 -> drop1 (in-place)
I1011 20:26:22.592736  4939 net.cpp:150] Setting up relu3
I1011 20:26:22.592743  4939 net.cpp:157] Top shape: 64 96 13 13 (1038336)
I1011 20:26:22.592746  4939 net.cpp:165] Memory required for data: 73900800
I1011 20:26:22.592747  4939 layer_factory.hpp:77] Creating layer conv4
I1011 20:26:22.592756  4939 net.cpp:100] Creating Layer conv4
I1011 20:26:22.592757  4939 net.cpp:434] conv4 <- drop1
I1011 20:26:22.592761  4939 net.cpp:408] conv4 -> conv4
I1011 20:26:22.594578  4939 net.cpp:150] Setting up conv4
I1011 20:26:22.594588  4939 net.cpp:157] Top shape: 64 192 11 11 (1486848)
I1011 20:26:22.594590  4939 net.cpp:165] Memory required for data: 79848192
I1011 20:26:22.594594  4939 layer_factory.hpp:77] Creating layer conv5
I1011 20:26:22.594601  4939 net.cpp:100] Creating Layer conv5
I1011 20:26:22.594604  4939 net.cpp:434] conv5 <- conv4
I1011 20:26:22.594607  4939 net.cpp:408] conv5 -> conv5
I1011 20:26:22.597043  4939 net.cpp:150] Setting up conv5
I1011 20:26:22.597053  4939 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1011 20:26:22.597054  4939 net.cpp:165] Memory required for data: 83829504
I1011 20:26:22.597059  4939 layer_factory.hpp:77] Creating layer bn2
I1011 20:26:22.597064  4939 net.cpp:100] Creating Layer bn2
I1011 20:26:22.597066  4939 net.cpp:434] bn2 <- conv5
I1011 20:26:22.597069  4939 net.cpp:408] bn2 -> bn2
I1011 20:26:22.597275  4939 net.cpp:150] Setting up bn2
I1011 20:26:22.597280  4939 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1011 20:26:22.597281  4939 net.cpp:165] Memory required for data: 87810816
I1011 20:26:22.597286  4939 layer_factory.hpp:77] Creating layer conv6
I1011 20:26:22.597291  4939 net.cpp:100] Creating Layer conv6
I1011 20:26:22.597293  4939 net.cpp:434] conv6 <- bn2
I1011 20:26:22.597298  4939 net.cpp:408] conv6 -> conv6
I1011 20:26:22.599805  4939 net.cpp:150] Setting up conv6
I1011 20:26:22.599815  4939 net.cpp:157] Top shape: 64 192 4 4 (196608)
I1011 20:26:22.599817  4939 net.cpp:165] Memory required for data: 88597248
I1011 20:26:22.599825  4939 layer_factory.hpp:77] Creating layer drop4
I1011 20:26:22.599831  4939 net.cpp:100] Creating Layer drop4
I1011 20:26:22.599833  4939 net.cpp:434] drop4 <- conv6
I1011 20:26:22.599838  4939 net.cpp:408] drop4 -> drop4
I1011 20:26:22.599882  4939 net.cpp:150] Setting up drop4
I1011 20:26:22.599900  4939 net.cpp:157] Top shape: 64 192 4 4 (196608)
I1011 20:26:22.599901  4939 net.cpp:165] Memory required for data: 89383680
I1011 20:26:22.599903  4939 layer_factory.hpp:77] Creating layer relu6
I1011 20:26:22.599907  4939 net.cpp:100] Creating Layer relu6
I1011 20:26:22.599931  4939 net.cpp:434] relu6 <- drop4
I1011 20:26:22.599936  4939 net.cpp:395] relu6 -> drop4 (in-place)
I1011 20:26:22.600203  4939 net.cpp:150] Setting up relu6
I1011 20:26:22.600210  4939 net.cpp:157] Top shape: 64 192 4 4 (196608)
I1011 20:26:22.600227  4939 net.cpp:165] Memory required for data: 90170112
I1011 20:26:22.600229  4939 layer_factory.hpp:77] Creating layer conv7
I1011 20:26:22.600235  4939 net.cpp:100] Creating Layer conv7
I1011 20:26:22.600239  4939 net.cpp:434] conv7 <- drop4
I1011 20:26:22.600242  4939 net.cpp:408] conv7 -> conv7
I1011 20:26:22.603214  4939 net.cpp:150] Setting up conv7
I1011 20:26:22.603231  4939 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1011 20:26:22.603235  4939 net.cpp:165] Memory required for data: 90366720
I1011 20:26:22.603241  4939 layer_factory.hpp:77] Creating layer relu7
I1011 20:26:22.603247  4939 net.cpp:100] Creating Layer relu7
I1011 20:26:22.603250  4939 net.cpp:434] relu7 <- conv7
I1011 20:26:22.603253  4939 net.cpp:395] relu7 -> conv7 (in-place)
I1011 20:26:22.603512  4939 net.cpp:150] Setting up relu7
I1011 20:26:22.603520  4939 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1011 20:26:22.603523  4939 net.cpp:165] Memory required for data: 90563328
I1011 20:26:22.603524  4939 layer_factory.hpp:77] Creating layer conv8
I1011 20:26:22.603533  4939 net.cpp:100] Creating Layer conv8
I1011 20:26:22.603536  4939 net.cpp:434] conv8 <- conv7
I1011 20:26:22.603540  4939 net.cpp:408] conv8 -> conv8
I1011 20:26:22.604784  4939 net.cpp:150] Setting up conv8
I1011 20:26:22.604802  4939 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1011 20:26:22.604804  4939 net.cpp:165] Memory required for data: 90759936
I1011 20:26:22.604809  4939 layer_factory.hpp:77] Creating layer relu8
I1011 20:26:22.604812  4939 net.cpp:100] Creating Layer relu8
I1011 20:26:22.604815  4939 net.cpp:434] relu8 <- conv8
I1011 20:26:22.604820  4939 net.cpp:395] relu8 -> conv8 (in-place)
I1011 20:26:22.605070  4939 net.cpp:150] Setting up relu8
I1011 20:26:22.605078  4939 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1011 20:26:22.605080  4939 net.cpp:165] Memory required for data: 90956544
I1011 20:26:22.605083  4939 layer_factory.hpp:77] Creating layer conv9
I1011 20:26:22.605088  4939 net.cpp:100] Creating Layer conv9
I1011 20:26:22.605092  4939 net.cpp:434] conv9 <- conv8
I1011 20:26:22.605095  4939 net.cpp:408] conv9 -> conv9
I1011 20:26:22.605913  4939 net.cpp:150] Setting up conv9
I1011 20:26:22.605923  4939 net.cpp:157] Top shape: 64 10 2 2 (2560)
I1011 20:26:22.605926  4939 net.cpp:165] Memory required for data: 90966784
I1011 20:26:22.605931  4939 layer_factory.hpp:77] Creating layer relu9
I1011 20:26:22.605934  4939 net.cpp:100] Creating Layer relu9
I1011 20:26:22.605937  4939 net.cpp:434] relu9 <- conv9
I1011 20:26:22.605939  4939 net.cpp:395] relu9 -> conv9 (in-place)
I1011 20:26:22.606196  4939 net.cpp:150] Setting up relu9
I1011 20:26:22.606205  4939 net.cpp:157] Top shape: 64 10 2 2 (2560)
I1011 20:26:22.606207  4939 net.cpp:165] Memory required for data: 90977024
I1011 20:26:22.606209  4939 layer_factory.hpp:77] Creating layer pool
I1011 20:26:22.606215  4939 net.cpp:100] Creating Layer pool
I1011 20:26:22.606216  4939 net.cpp:434] pool <- conv9
I1011 20:26:22.606220  4939 net.cpp:408] pool -> pool
I1011 20:26:22.606415  4939 net.cpp:150] Setting up pool
I1011 20:26:22.606420  4939 net.cpp:157] Top shape: 64 10 1 1 (640)
I1011 20:26:22.606422  4939 net.cpp:165] Memory required for data: 90979584
I1011 20:26:22.606425  4939 layer_factory.hpp:77] Creating layer flatten
I1011 20:26:22.606427  4939 net.cpp:100] Creating Layer flatten
I1011 20:26:22.606429  4939 net.cpp:434] flatten <- pool
I1011 20:26:22.606433  4939 net.cpp:408] flatten -> flatten
I1011 20:26:22.606453  4939 net.cpp:150] Setting up flatten
I1011 20:26:22.606472  4939 net.cpp:157] Top shape: 64 10 (640)
I1011 20:26:22.606473  4939 net.cpp:165] Memory required for data: 90982144
I1011 20:26:22.606475  4939 layer_factory.hpp:77] Creating layer score
I1011 20:26:22.606479  4939 net.cpp:100] Creating Layer score
I1011 20:26:22.606482  4939 net.cpp:434] score <- flatten
I1011 20:26:22.606485  4939 net.cpp:408] score -> score
I1011 20:26:22.606619  4939 net.cpp:150] Setting up score
I1011 20:26:22.606624  4939 net.cpp:157] Top shape: 64 10 (640)
I1011 20:26:22.606626  4939 net.cpp:165] Memory required for data: 90984704
I1011 20:26:22.606629  4939 layer_factory.hpp:77] Creating layer score_score_0_split
I1011 20:26:22.606634  4939 net.cpp:100] Creating Layer score_score_0_split
I1011 20:26:22.606637  4939 net.cpp:434] score_score_0_split <- score
I1011 20:26:22.606640  4939 net.cpp:408] score_score_0_split -> score_score_0_split_0
I1011 20:26:22.606644  4939 net.cpp:408] score_score_0_split -> score_score_0_split_1
I1011 20:26:22.606699  4939 net.cpp:150] Setting up score_score_0_split
I1011 20:26:22.606705  4939 net.cpp:157] Top shape: 64 10 (640)
I1011 20:26:22.606708  4939 net.cpp:157] Top shape: 64 10 (640)
I1011 20:26:22.606709  4939 net.cpp:165] Memory required for data: 90989824
I1011 20:26:22.606711  4939 layer_factory.hpp:77] Creating layer accuracy
I1011 20:26:22.606715  4939 net.cpp:100] Creating Layer accuracy
I1011 20:26:22.606717  4939 net.cpp:434] accuracy <- score_score_0_split_0
I1011 20:26:22.606720  4939 net.cpp:434] accuracy <- label_data_1_split_0
I1011 20:26:22.606724  4939 net.cpp:408] accuracy -> accuracy
I1011 20:26:22.606729  4939 net.cpp:150] Setting up accuracy
I1011 20:26:22.606731  4939 net.cpp:157] Top shape: (1)
I1011 20:26:22.606747  4939 net.cpp:165] Memory required for data: 90989828
I1011 20:26:22.606756  4939 layer_factory.hpp:77] Creating layer loss
I1011 20:26:22.606762  4939 net.cpp:100] Creating Layer loss
I1011 20:26:22.606777  4939 net.cpp:434] loss <- score_score_0_split_1
I1011 20:26:22.606781  4939 net.cpp:434] loss <- label_data_1_split_1
I1011 20:26:22.606783  4939 net.cpp:408] loss -> loss
I1011 20:26:22.606806  4939 layer_factory.hpp:77] Creating layer loss
I1011 20:26:22.607127  4939 net.cpp:150] Setting up loss
I1011 20:26:22.607136  4939 net.cpp:157] Top shape: (1)
I1011 20:26:22.607137  4939 net.cpp:160]     with loss weight 1
I1011 20:26:22.607148  4939 net.cpp:165] Memory required for data: 90989832
I1011 20:26:22.607151  4939 net.cpp:226] loss needs backward computation.
I1011 20:26:22.607156  4939 net.cpp:228] accuracy does not need backward computation.
I1011 20:26:22.607161  4939 net.cpp:226] score_score_0_split needs backward computation.
I1011 20:26:22.607162  4939 net.cpp:226] score needs backward computation.
I1011 20:26:22.607164  4939 net.cpp:226] flatten needs backward computation.
I1011 20:26:22.607167  4939 net.cpp:226] pool needs backward computation.
I1011 20:26:22.607168  4939 net.cpp:226] relu9 needs backward computation.
I1011 20:26:22.607169  4939 net.cpp:226] conv9 needs backward computation.
I1011 20:26:22.607187  4939 net.cpp:226] relu8 needs backward computation.
I1011 20:26:22.607188  4939 net.cpp:226] conv8 needs backward computation.
I1011 20:26:22.607190  4939 net.cpp:226] relu7 needs backward computation.
I1011 20:26:22.607192  4939 net.cpp:226] conv7 needs backward computation.
I1011 20:26:22.607194  4939 net.cpp:226] relu6 needs backward computation.
I1011 20:26:22.607197  4939 net.cpp:226] drop4 needs backward computation.
I1011 20:26:22.607197  4939 net.cpp:226] conv6 needs backward computation.
I1011 20:26:22.607200  4939 net.cpp:226] bn2 needs backward computation.
I1011 20:26:22.607203  4939 net.cpp:226] conv5 needs backward computation.
I1011 20:26:22.607204  4939 net.cpp:226] conv4 needs backward computation.
I1011 20:26:22.607221  4939 net.cpp:226] relu3 needs backward computation.
I1011 20:26:22.607223  4939 net.cpp:226] drop1 needs backward computation.
I1011 20:26:22.607225  4939 net.cpp:226] conv3 needs backward computation.
I1011 20:26:22.607228  4939 net.cpp:226] bn1 needs backward computation.
I1011 20:26:22.607242  4939 net.cpp:226] conv2 needs backward computation.
I1011 20:26:22.607245  4939 net.cpp:226] conv1 needs backward computation.
I1011 20:26:22.607247  4939 net.cpp:228] label_data_1_split does not need backward computation.
I1011 20:26:22.607250  4939 net.cpp:228] data does not need backward computation.
I1011 20:26:22.607270  4939 net.cpp:270] This network produces output accuracy
I1011 20:26:22.607272  4939 net.cpp:270] This network produces output loss
I1011 20:26:22.607301  4939 net.cpp:283] Network initialization done.
I1011 20:26:22.607512  4939 solver.cpp:181] Creating test net (#0) specified by test_net file: /home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_test.prototxt
I1011 20:26:22.607650  4939 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "../../../data/cifar-10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "bn1"
  top: "conv3"
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "conv3"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "drop1"
  top: "drop1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "drop1"
  top: "conv4"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv5"
  top: "bn2"
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "bn2"
  top: "conv6"
  convolution_param {
    num_output: 192
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "conv6"
  top: "drop4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "drop4"
  top: "drop4"
}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "drop4"
  top: "conv7"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "conv7"
  top: "conv8"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "conv8"
  top: "conv9"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}
layer {
  name: "pool"
  type: "Pooling"
  bottom: "conv9"
  top: "pool"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "flatten"
  type: "Flatten"
  bottom: "pool"
  top: "flatten"
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "flatten"
  top: "score"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
I1011 20:26:22.607749  4939 layer_factory.hpp:77] Creating layer data
I1011 20:26:22.607846  4939 net.cpp:100] Creating Layer data
I1011 20:26:22.607853  4939 net.cpp:408] data -> data
I1011 20:26:22.607859  4939 net.cpp:408] data -> label
I1011 20:26:22.607866  4939 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I1011 20:26:22.608734  4946 db_lmdb.cpp:35] Opened lmdb ../../../data/cifar-10/cifar10_test_lmdb
I1011 20:26:22.608875  4939 data_layer.cpp:41] output data size: 100,3,32,32
I1011 20:26:22.610827  4939 net.cpp:150] Setting up data
I1011 20:26:22.610869  4939 net.cpp:157] Top shape: 100 3 32 32 (307200)
I1011 20:26:22.610873  4939 net.cpp:157] Top shape: 100 (100)
I1011 20:26:22.610875  4939 net.cpp:165] Memory required for data: 1229200
I1011 20:26:22.610878  4939 layer_factory.hpp:77] Creating layer label_data_1_split
I1011 20:26:22.610888  4939 net.cpp:100] Creating Layer label_data_1_split
I1011 20:26:22.610890  4939 net.cpp:434] label_data_1_split <- label
I1011 20:26:22.610909  4939 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1011 20:26:22.610930  4939 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1011 20:26:22.611063  4939 net.cpp:150] Setting up label_data_1_split
I1011 20:26:22.611068  4939 net.cpp:157] Top shape: 100 (100)
I1011 20:26:22.611069  4939 net.cpp:157] Top shape: 100 (100)
I1011 20:26:22.611071  4939 net.cpp:165] Memory required for data: 1230000
I1011 20:26:22.611096  4939 layer_factory.hpp:77] Creating layer conv1
I1011 20:26:22.611105  4939 net.cpp:100] Creating Layer conv1
I1011 20:26:22.611107  4939 net.cpp:434] conv1 <- data
I1011 20:26:22.611112  4939 net.cpp:408] conv1 -> conv1
I1011 20:26:22.612509  4939 net.cpp:150] Setting up conv1
I1011 20:26:22.612535  4939 net.cpp:157] Top shape: 100 96 30 30 (8640000)
I1011 20:26:22.612540  4939 net.cpp:165] Memory required for data: 35790000
I1011 20:26:22.612563  4939 layer_factory.hpp:77] Creating layer conv2
I1011 20:26:22.612572  4939 net.cpp:100] Creating Layer conv2
I1011 20:26:22.612581  4939 net.cpp:434] conv2 <- conv1
I1011 20:26:22.612586  4939 net.cpp:408] conv2 -> conv2
I1011 20:26:22.613984  4939 net.cpp:150] Setting up conv2
I1011 20:26:22.614013  4939 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1011 20:26:22.614017  4939 net.cpp:165] Memory required for data: 65895600
I1011 20:26:22.614037  4939 layer_factory.hpp:77] Creating layer bn1
I1011 20:26:22.614047  4939 net.cpp:100] Creating Layer bn1
I1011 20:26:22.614053  4939 net.cpp:434] bn1 <- conv2
I1011 20:26:22.614056  4939 net.cpp:408] bn1 -> bn1
I1011 20:26:22.614311  4939 net.cpp:150] Setting up bn1
I1011 20:26:22.614318  4939 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1011 20:26:22.614321  4939 net.cpp:165] Memory required for data: 96001200
I1011 20:26:22.614327  4939 layer_factory.hpp:77] Creating layer conv3
I1011 20:26:22.614336  4939 net.cpp:100] Creating Layer conv3
I1011 20:26:22.614337  4939 net.cpp:434] conv3 <- bn1
I1011 20:26:22.614341  4939 net.cpp:408] conv3 -> conv3
I1011 20:26:22.615677  4939 net.cpp:150] Setting up conv3
I1011 20:26:22.615689  4939 net.cpp:157] Top shape: 100 96 13 13 (1622400)
I1011 20:26:22.615694  4939 net.cpp:165] Memory required for data: 102490800
I1011 20:26:22.615700  4939 layer_factory.hpp:77] Creating layer drop1
I1011 20:26:22.615707  4939 net.cpp:100] Creating Layer drop1
I1011 20:26:22.615711  4939 net.cpp:434] drop1 <- conv3
I1011 20:26:22.615715  4939 net.cpp:408] drop1 -> drop1
I1011 20:26:22.615752  4939 net.cpp:150] Setting up drop1
I1011 20:26:22.615758  4939 net.cpp:157] Top shape: 100 96 13 13 (1622400)
I1011 20:26:22.615761  4939 net.cpp:165] Memory required for data: 108980400
I1011 20:26:22.615762  4939 layer_factory.hpp:77] Creating layer relu3
I1011 20:26:22.615767  4939 net.cpp:100] Creating Layer relu3
I1011 20:26:22.615775  4939 net.cpp:434] relu3 <- drop1
I1011 20:26:22.615778  4939 net.cpp:395] relu3 -> drop1 (in-place)
I1011 20:26:22.616030  4939 net.cpp:150] Setting up relu3
I1011 20:26:22.616039  4939 net.cpp:157] Top shape: 100 96 13 13 (1622400)
I1011 20:26:22.616042  4939 net.cpp:165] Memory required for data: 115470000
I1011 20:26:22.616044  4939 layer_factory.hpp:77] Creating layer conv4
I1011 20:26:22.616053  4939 net.cpp:100] Creating Layer conv4
I1011 20:26:22.616056  4939 net.cpp:434] conv4 <- drop1
I1011 20:26:22.616060  4939 net.cpp:408] conv4 -> conv4
I1011 20:26:22.618268  4939 net.cpp:150] Setting up conv4
I1011 20:26:22.618283  4939 net.cpp:157] Top shape: 100 192 11 11 (2323200)
I1011 20:26:22.618285  4939 net.cpp:165] Memory required for data: 124762800
I1011 20:26:22.618305  4939 layer_factory.hpp:77] Creating layer conv5
I1011 20:26:22.618314  4939 net.cpp:100] Creating Layer conv5
I1011 20:26:22.618317  4939 net.cpp:434] conv5 <- conv4
I1011 20:26:22.618322  4939 net.cpp:408] conv5 -> conv5
I1011 20:26:22.621014  4939 net.cpp:150] Setting up conv5
I1011 20:26:22.621031  4939 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1011 20:26:22.621033  4939 net.cpp:165] Memory required for data: 130983600
I1011 20:26:22.621052  4939 layer_factory.hpp:77] Creating layer bn2
I1011 20:26:22.621070  4939 net.cpp:100] Creating Layer bn2
I1011 20:26:22.621073  4939 net.cpp:434] bn2 <- conv5
I1011 20:26:22.621078  4939 net.cpp:408] bn2 -> bn2
I1011 20:26:22.621292  4939 net.cpp:150] Setting up bn2
I1011 20:26:22.621297  4939 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1011 20:26:22.621299  4939 net.cpp:165] Memory required for data: 137204400
I1011 20:26:22.621304  4939 layer_factory.hpp:77] Creating layer conv6
I1011 20:26:22.621309  4939 net.cpp:100] Creating Layer conv6
I1011 20:26:22.621312  4939 net.cpp:434] conv6 <- bn2
I1011 20:26:22.621316  4939 net.cpp:408] conv6 -> conv6
I1011 20:26:22.623910  4939 net.cpp:150] Setting up conv6
I1011 20:26:22.623921  4939 net.cpp:157] Top shape: 100 192 4 4 (307200)
I1011 20:26:22.623924  4939 net.cpp:165] Memory required for data: 138433200
I1011 20:26:22.623932  4939 layer_factory.hpp:77] Creating layer drop4
I1011 20:26:22.623939  4939 net.cpp:100] Creating Layer drop4
I1011 20:26:22.623940  4939 net.cpp:434] drop4 <- conv6
I1011 20:26:22.623944  4939 net.cpp:408] drop4 -> drop4
I1011 20:26:22.624006  4939 net.cpp:150] Setting up drop4
I1011 20:26:22.624024  4939 net.cpp:157] Top shape: 100 192 4 4 (307200)
I1011 20:26:22.624027  4939 net.cpp:165] Memory required for data: 139662000
I1011 20:26:22.624028  4939 layer_factory.hpp:77] Creating layer relu6
I1011 20:26:22.624032  4939 net.cpp:100] Creating Layer relu6
I1011 20:26:22.624048  4939 net.cpp:434] relu6 <- drop4
I1011 20:26:22.624052  4939 net.cpp:395] relu6 -> drop4 (in-place)
I1011 20:26:22.624325  4939 net.cpp:150] Setting up relu6
I1011 20:26:22.624332  4939 net.cpp:157] Top shape: 100 192 4 4 (307200)
I1011 20:26:22.624335  4939 net.cpp:165] Memory required for data: 140890800
I1011 20:26:22.624336  4939 layer_factory.hpp:77] Creating layer conv7
I1011 20:26:22.624342  4939 net.cpp:100] Creating Layer conv7
I1011 20:26:22.624346  4939 net.cpp:434] conv7 <- drop4
I1011 20:26:22.624348  4939 net.cpp:408] conv7 -> conv7
I1011 20:26:22.626983  4939 net.cpp:150] Setting up conv7
I1011 20:26:22.626994  4939 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1011 20:26:22.626996  4939 net.cpp:165] Memory required for data: 141198000
I1011 20:26:22.627002  4939 layer_factory.hpp:77] Creating layer relu7
I1011 20:26:22.627005  4939 net.cpp:100] Creating Layer relu7
I1011 20:26:22.627007  4939 net.cpp:434] relu7 <- conv7
I1011 20:26:22.627012  4939 net.cpp:395] relu7 -> conv7 (in-place)
I1011 20:26:22.627282  4939 net.cpp:150] Setting up relu7
I1011 20:26:22.627290  4939 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1011 20:26:22.627292  4939 net.cpp:165] Memory required for data: 141505200
I1011 20:26:22.627295  4939 layer_factory.hpp:77] Creating layer conv8
I1011 20:26:22.627303  4939 net.cpp:100] Creating Layer conv8
I1011 20:26:22.627306  4939 net.cpp:434] conv8 <- conv7
I1011 20:26:22.627310  4939 net.cpp:408] conv8 -> conv8
I1011 20:26:22.628317  4939 net.cpp:150] Setting up conv8
I1011 20:26:22.628326  4939 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1011 20:26:22.628329  4939 net.cpp:165] Memory required for data: 141812400
I1011 20:26:22.628334  4939 layer_factory.hpp:77] Creating layer relu8
I1011 20:26:22.628336  4939 net.cpp:100] Creating Layer relu8
I1011 20:26:22.628339  4939 net.cpp:434] relu8 <- conv8
I1011 20:26:22.628342  4939 net.cpp:395] relu8 -> conv8 (in-place)
I1011 20:26:22.628517  4939 net.cpp:150] Setting up relu8
I1011 20:26:22.628523  4939 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1011 20:26:22.628525  4939 net.cpp:165] Memory required for data: 142119600
I1011 20:26:22.628528  4939 layer_factory.hpp:77] Creating layer conv9
I1011 20:26:22.628533  4939 net.cpp:100] Creating Layer conv9
I1011 20:26:22.628535  4939 net.cpp:434] conv9 <- conv8
I1011 20:26:22.628540  4939 net.cpp:408] conv9 -> conv9
I1011 20:26:22.629390  4939 net.cpp:150] Setting up conv9
I1011 20:26:22.629398  4939 net.cpp:157] Top shape: 100 10 2 2 (4000)
I1011 20:26:22.629400  4939 net.cpp:165] Memory required for data: 142135600
I1011 20:26:22.629405  4939 layer_factory.hpp:77] Creating layer relu9
I1011 20:26:22.629410  4939 net.cpp:100] Creating Layer relu9
I1011 20:26:22.629411  4939 net.cpp:434] relu9 <- conv9
I1011 20:26:22.629415  4939 net.cpp:395] relu9 -> conv9 (in-place)
I1011 20:26:22.629673  4939 net.cpp:150] Setting up relu9
I1011 20:26:22.629680  4939 net.cpp:157] Top shape: 100 10 2 2 (4000)
I1011 20:26:22.629683  4939 net.cpp:165] Memory required for data: 142151600
I1011 20:26:22.629685  4939 layer_factory.hpp:77] Creating layer pool
I1011 20:26:22.629690  4939 net.cpp:100] Creating Layer pool
I1011 20:26:22.629693  4939 net.cpp:434] pool <- conv9
I1011 20:26:22.629696  4939 net.cpp:408] pool -> pool
I1011 20:26:22.629894  4939 net.cpp:150] Setting up pool
I1011 20:26:22.629899  4939 net.cpp:157] Top shape: 100 10 1 1 (1000)
I1011 20:26:22.629901  4939 net.cpp:165] Memory required for data: 142155600
I1011 20:26:22.629904  4939 layer_factory.hpp:77] Creating layer flatten
I1011 20:26:22.629907  4939 net.cpp:100] Creating Layer flatten
I1011 20:26:22.629909  4939 net.cpp:434] flatten <- pool
I1011 20:26:22.629912  4939 net.cpp:408] flatten -> flatten
I1011 20:26:22.629950  4939 net.cpp:150] Setting up flatten
I1011 20:26:22.629954  4939 net.cpp:157] Top shape: 100 10 (1000)
I1011 20:26:22.629956  4939 net.cpp:165] Memory required for data: 142159600
I1011 20:26:22.629958  4939 layer_factory.hpp:77] Creating layer score
I1011 20:26:22.629976  4939 net.cpp:100] Creating Layer score
I1011 20:26:22.629979  4939 net.cpp:434] score <- flatten
I1011 20:26:22.629983  4939 net.cpp:408] score -> score
I1011 20:26:22.630106  4939 net.cpp:150] Setting up score
I1011 20:26:22.630111  4939 net.cpp:157] Top shape: 100 10 (1000)
I1011 20:26:22.630113  4939 net.cpp:165] Memory required for data: 142163600
I1011 20:26:22.630131  4939 layer_factory.hpp:77] Creating layer score_score_0_split
I1011 20:26:22.630137  4939 net.cpp:100] Creating Layer score_score_0_split
I1011 20:26:22.630141  4939 net.cpp:434] score_score_0_split <- score
I1011 20:26:22.630144  4939 net.cpp:408] score_score_0_split -> score_score_0_split_0
I1011 20:26:22.630164  4939 net.cpp:408] score_score_0_split -> score_score_0_split_1
I1011 20:26:22.630214  4939 net.cpp:150] Setting up score_score_0_split
I1011 20:26:22.630219  4939 net.cpp:157] Top shape: 100 10 (1000)
I1011 20:26:22.630223  4939 net.cpp:157] Top shape: 100 10 (1000)
I1011 20:26:22.630224  4939 net.cpp:165] Memory required for data: 142171600
I1011 20:26:22.630226  4939 layer_factory.hpp:77] Creating layer accuracy
I1011 20:26:22.630231  4939 net.cpp:100] Creating Layer accuracy
I1011 20:26:22.630234  4939 net.cpp:434] accuracy <- score_score_0_split_0
I1011 20:26:22.630237  4939 net.cpp:434] accuracy <- label_data_1_split_0
I1011 20:26:22.630241  4939 net.cpp:408] accuracy -> accuracy
I1011 20:26:22.630269  4939 net.cpp:150] Setting up accuracy
I1011 20:26:22.630272  4939 net.cpp:157] Top shape: (1)
I1011 20:26:22.630275  4939 net.cpp:165] Memory required for data: 142171604
I1011 20:26:22.630276  4939 layer_factory.hpp:77] Creating layer loss
I1011 20:26:22.630280  4939 net.cpp:100] Creating Layer loss
I1011 20:26:22.630282  4939 net.cpp:434] loss <- score_score_0_split_1
I1011 20:26:22.630285  4939 net.cpp:434] loss <- label_data_1_split_1
I1011 20:26:22.630302  4939 net.cpp:408] loss -> loss
I1011 20:26:22.630308  4939 layer_factory.hpp:77] Creating layer loss
I1011 20:26:22.630630  4939 net.cpp:150] Setting up loss
I1011 20:26:22.630638  4939 net.cpp:157] Top shape: (1)
I1011 20:26:22.630641  4939 net.cpp:160]     with loss weight 1
I1011 20:26:22.630664  4939 net.cpp:165] Memory required for data: 142171608
I1011 20:26:22.630666  4939 net.cpp:226] loss needs backward computation.
I1011 20:26:22.630671  4939 net.cpp:228] accuracy does not need backward computation.
I1011 20:26:22.630673  4939 net.cpp:226] score_score_0_split needs backward computation.
I1011 20:26:22.630676  4939 net.cpp:226] score needs backward computation.
I1011 20:26:22.630677  4939 net.cpp:226] flatten needs backward computation.
I1011 20:26:22.630679  4939 net.cpp:226] pool needs backward computation.
I1011 20:26:22.630682  4939 net.cpp:226] relu9 needs backward computation.
I1011 20:26:22.630684  4939 net.cpp:226] conv9 needs backward computation.
I1011 20:26:22.630686  4939 net.cpp:226] relu8 needs backward computation.
I1011 20:26:22.630688  4939 net.cpp:226] conv8 needs backward computation.
I1011 20:26:22.630691  4939 net.cpp:226] relu7 needs backward computation.
I1011 20:26:22.630692  4939 net.cpp:226] conv7 needs backward computation.
I1011 20:26:22.630694  4939 net.cpp:226] relu6 needs backward computation.
I1011 20:26:22.630709  4939 net.cpp:226] drop4 needs backward computation.
I1011 20:26:22.630712  4939 net.cpp:226] conv6 needs backward computation.
I1011 20:26:22.630722  4939 net.cpp:226] bn2 needs backward computation.
I1011 20:26:22.630725  4939 net.cpp:226] conv5 needs backward computation.
I1011 20:26:22.630728  4939 net.cpp:226] conv4 needs backward computation.
I1011 20:26:22.630730  4939 net.cpp:226] relu3 needs backward computation.
I1011 20:26:22.630733  4939 net.cpp:226] drop1 needs backward computation.
I1011 20:26:22.630734  4939 net.cpp:226] conv3 needs backward computation.
I1011 20:26:22.630738  4939 net.cpp:226] bn1 needs backward computation.
I1011 20:26:22.630739  4939 net.cpp:226] conv2 needs backward computation.
I1011 20:26:22.630741  4939 net.cpp:226] conv1 needs backward computation.
I1011 20:26:22.630744  4939 net.cpp:228] label_data_1_split does not need backward computation.
I1011 20:26:22.630748  4939 net.cpp:228] data does not need backward computation.
I1011 20:26:22.630749  4939 net.cpp:270] This network produces output accuracy
I1011 20:26:22.630753  4939 net.cpp:270] This network produces output loss
I1011 20:26:22.630785  4939 net.cpp:283] Network initialization done.
I1011 20:26:22.630858  4939 solver.cpp:60] Solver scaffolding done.
I1011 20:26:22.631935  4939 caffe.cpp:251] Starting Optimization
I1011 20:26:22.631940  4939 solver.cpp:279] Solving 
I1011 20:26:22.631942  4939 solver.cpp:280] Learning Rate Policy: step
I1011 20:26:22.632863  4939 solver.cpp:337] Iteration 0, Testing net (#0)
I1011 20:26:25.841332  4939 solver.cpp:404]     Test net output #0: accuracy = 0.0886
I1011 20:26:25.841356  4939 solver.cpp:404]     Test net output #1: loss = 79.5985 (* 1 = 79.5985 loss)
I1011 20:26:25.876346  4939 solver.cpp:228] Iteration 0, loss = 2.58496
I1011 20:26:25.876371  4939 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1011 20:26:25.876379  4939 solver.cpp:244]     Train net output #1: loss = 2.58496 (* 1 = 2.58496 loss)
I1011 20:26:25.876386  4939 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I1011 20:26:29.763207  4939 solver.cpp:228] Iteration 50, loss = 2.2124
I1011 20:26:29.763228  4939 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I1011 20:26:29.763234  4939 solver.cpp:244]     Train net output #1: loss = 2.2124 (* 1 = 2.2124 loss)
I1011 20:26:29.763238  4939 sgd_solver.cpp:106] Iteration 50, lr = 0.0001
I1011 20:26:33.646541  4939 solver.cpp:228] Iteration 100, loss = 2.0387
I1011 20:26:33.646562  4939 solver.cpp:244]     Train net output #0: accuracy = 0.234375
I1011 20:26:33.646569  4939 solver.cpp:244]     Train net output #1: loss = 2.0387 (* 1 = 2.0387 loss)
I1011 20:26:33.646589  4939 sgd_solver.cpp:106] Iteration 100, lr = 0.0001
I1011 20:26:37.532272  4939 solver.cpp:228] Iteration 150, loss = 2.05198
I1011 20:26:37.532292  4939 solver.cpp:244]     Train net output #0: accuracy = 0.25
I1011 20:26:37.532313  4939 solver.cpp:244]     Train net output #1: loss = 2.05198 (* 1 = 2.05198 loss)
I1011 20:26:37.532317  4939 sgd_solver.cpp:106] Iteration 150, lr = 0.0001
I1011 20:26:41.391482  4939 solver.cpp:337] Iteration 200, Testing net (#0)
I1011 20:26:44.581836  4939 solver.cpp:404]     Test net output #0: accuracy = 0.2812
I1011 20:26:44.581859  4939 solver.cpp:404]     Test net output #1: loss = 1.98711 (* 1 = 1.98711 loss)
I1011 20:26:44.606199  4939 solver.cpp:228] Iteration 200, loss = 1.88645
I1011 20:26:44.606217  4939 solver.cpp:244]     Train net output #0: accuracy = 0.234375
I1011 20:26:44.606223  4939 solver.cpp:244]     Train net output #1: loss = 1.88645 (* 1 = 1.88645 loss)
I1011 20:26:44.606228  4939 sgd_solver.cpp:106] Iteration 200, lr = 0.0001
I1011 20:26:48.492010  4939 solver.cpp:228] Iteration 250, loss = 1.93927
I1011 20:26:48.492032  4939 solver.cpp:244]     Train net output #0: accuracy = 0.28125
I1011 20:26:48.492038  4939 solver.cpp:244]     Train net output #1: loss = 1.93927 (* 1 = 1.93927 loss)
I1011 20:26:48.492043  4939 sgd_solver.cpp:106] Iteration 250, lr = 0.0001
I1011 20:26:52.380475  4939 solver.cpp:228] Iteration 300, loss = 2.02227
I1011 20:26:52.380592  4939 solver.cpp:244]     Train net output #0: accuracy = 0.25
I1011 20:26:52.380602  4939 solver.cpp:244]     Train net output #1: loss = 2.02227 (* 1 = 2.02227 loss)
I1011 20:26:52.380606  4939 sgd_solver.cpp:106] Iteration 300, lr = 0.0001
I1011 20:26:56.267942  4939 solver.cpp:228] Iteration 350, loss = 2.09362
I1011 20:26:56.267979  4939 solver.cpp:244]     Train net output #0: accuracy = 0.28125
I1011 20:26:56.267987  4939 solver.cpp:244]     Train net output #1: loss = 2.09362 (* 1 = 2.09362 loss)
I1011 20:26:56.267992  4939 sgd_solver.cpp:106] Iteration 350, lr = 0.0001
I1011 20:27:00.132992  4939 solver.cpp:337] Iteration 400, Testing net (#0)
I1011 20:27:03.325398  4939 solver.cpp:404]     Test net output #0: accuracy = 0.3434
I1011 20:27:03.325433  4939 solver.cpp:404]     Test net output #1: loss = 1.86396 (* 1 = 1.86396 loss)
I1011 20:27:03.349781  4939 solver.cpp:228] Iteration 400, loss = 2.04791
I1011 20:27:03.349798  4939 solver.cpp:244]     Train net output #0: accuracy = 0.328125
I1011 20:27:03.349819  4939 solver.cpp:244]     Train net output #1: loss = 2.04791 (* 1 = 2.04791 loss)
I1011 20:27:03.349824  4939 sgd_solver.cpp:106] Iteration 400, lr = 0.0001
I1011 20:27:07.236340  4939 solver.cpp:228] Iteration 450, loss = 1.65127
I1011 20:27:07.236359  4939 solver.cpp:244]     Train net output #0: accuracy = 0.46875
I1011 20:27:07.236367  4939 solver.cpp:244]     Train net output #1: loss = 1.65127 (* 1 = 1.65127 loss)
I1011 20:27:07.236371  4939 sgd_solver.cpp:106] Iteration 450, lr = 0.0001
I1011 20:27:11.124218  4939 solver.cpp:228] Iteration 500, loss = 2.01189
I1011 20:27:11.124240  4939 solver.cpp:244]     Train net output #0: accuracy = 0.28125
I1011 20:27:11.124248  4939 solver.cpp:244]     Train net output #1: loss = 2.01189 (* 1 = 2.01189 loss)
I1011 20:27:11.124251  4939 sgd_solver.cpp:106] Iteration 500, lr = 0.0001
I1011 20:27:15.010952  4939 solver.cpp:228] Iteration 550, loss = 1.76729
I1011 20:27:15.010972  4939 solver.cpp:244]     Train net output #0: accuracy = 0.390625
I1011 20:27:15.010994  4939 solver.cpp:244]     Train net output #1: loss = 1.76729 (* 1 = 1.76729 loss)
I1011 20:27:15.010998  4939 sgd_solver.cpp:106] Iteration 550, lr = 0.0001
I1011 20:27:18.874857  4939 solver.cpp:337] Iteration 600, Testing net (#0)
I1011 20:27:22.065256  4939 solver.cpp:404]     Test net output #0: accuracy = 0.356
I1011 20:27:22.065292  4939 solver.cpp:404]     Test net output #1: loss = 1.82188 (* 1 = 1.82188 loss)
I1011 20:27:22.089607  4939 solver.cpp:228] Iteration 600, loss = 2.05071
I1011 20:27:22.089624  4939 solver.cpp:244]     Train net output #0: accuracy = 0.34375
I1011 20:27:22.089630  4939 solver.cpp:244]     Train net output #1: loss = 2.05071 (* 1 = 2.05071 loss)
I1011 20:27:22.089634  4939 sgd_solver.cpp:106] Iteration 600, lr = 0.0001
I1011 20:27:25.976574  4939 solver.cpp:228] Iteration 650, loss = 1.73697
I1011 20:27:25.976647  4939 solver.cpp:244]     Train net output #0: accuracy = 0.390625
I1011 20:27:25.976668  4939 solver.cpp:244]     Train net output #1: loss = 1.73697 (* 1 = 1.73697 loss)
I1011 20:27:25.976671  4939 sgd_solver.cpp:106] Iteration 650, lr = 0.0001
I1011 20:27:29.866542  4939 solver.cpp:228] Iteration 700, loss = 1.86352
I1011 20:27:29.866564  4939 solver.cpp:244]     Train net output #0: accuracy = 0.28125
I1011 20:27:29.866570  4939 solver.cpp:244]     Train net output #1: loss = 1.86352 (* 1 = 1.86352 loss)
I1011 20:27:29.866575  4939 sgd_solver.cpp:106] Iteration 700, lr = 0.0001
I1011 20:27:33.755815  4939 solver.cpp:228] Iteration 750, loss = 1.91663
I1011 20:27:33.755847  4939 solver.cpp:244]     Train net output #0: accuracy = 0.359375
I1011 20:27:33.755856  4939 solver.cpp:244]     Train net output #1: loss = 1.91663 (* 1 = 1.91663 loss)
I1011 20:27:33.755858  4939 sgd_solver.cpp:106] Iteration 750, lr = 0.0001
I1011 20:27:37.619051  4939 solver.cpp:337] Iteration 800, Testing net (#0)
I1011 20:27:40.812957  4939 solver.cpp:404]     Test net output #0: accuracy = 0.4099
I1011 20:27:40.812994  4939 solver.cpp:404]     Test net output #1: loss = 1.69373 (* 1 = 1.69373 loss)
I1011 20:27:40.837529  4939 solver.cpp:228] Iteration 800, loss = 1.61061
I1011 20:27:40.837549  4939 solver.cpp:244]     Train net output #0: accuracy = 0.453125
I1011 20:27:40.837558  4939 solver.cpp:244]     Train net output #1: loss = 1.61061 (* 1 = 1.61061 loss)
I1011 20:27:40.837561  4939 sgd_solver.cpp:106] Iteration 800, lr = 0.0001
I1011 20:27:44.722865  4939 solver.cpp:228] Iteration 850, loss = 1.73622
I1011 20:27:44.722885  4939 solver.cpp:244]     Train net output #0: accuracy = 0.421875
I1011 20:27:44.722908  4939 solver.cpp:244]     Train net output #1: loss = 1.73622 (* 1 = 1.73622 loss)
I1011 20:27:44.722925  4939 sgd_solver.cpp:106] Iteration 850, lr = 0.0001
I1011 20:27:48.610601  4939 solver.cpp:228] Iteration 900, loss = 1.75052
I1011 20:27:48.610622  4939 solver.cpp:244]     Train net output #0: accuracy = 0.375
I1011 20:27:48.610630  4939 solver.cpp:244]     Train net output #1: loss = 1.75052 (* 1 = 1.75052 loss)
I1011 20:27:48.610632  4939 sgd_solver.cpp:106] Iteration 900, lr = 0.0001
