I1010 14:31:08.761183  5799 caffe.cpp:217] Using GPUs 0
I1010 14:31:08.763875  5799 caffe.cpp:222] GPU 0: GeForce GTX 750 Ti
I1010 14:31:08.871776  5799 solver.cpp:48] Initializing solver from parameters: 
train_net: "/home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_train.prototxt"
test_net: "/home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_test.prototxt"
test_iter: 100
test_interval: 250
base_lr: 0.0001
display: 50
max_iter: 2500
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 5000
snapshot_prefix: "cifar-10_relu_AdaDelta"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
type: "AdaDelta"
I1010 14:31:08.871940  5799 solver.cpp:81] Creating training net from train_net file: /home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_train.prototxt
I1010 14:31:08.872264  5799 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "../../../data/cifar-10/cifar10_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn1"
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "bn1"
  top: "bn1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "bn1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv5"
  top: "bn2"
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "bn2"
  top: "bn2"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "bn2"
  top: "drop4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "drop4"
  top: "conv6"
  convolution_param {
    num_output: 192
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "conv6"
  top: "conv6"
}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "conv6"
  top: "conv7"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "conv7"
  top: "conv8"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "conv8"
  top: "conv9"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}
layer {
  name: "pool"
  type: "Pooling"
  bottom: "conv9"
  top: "pool"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "flatten"
  type: "Flatten"
  bottom: "pool"
  top: "flatten"
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "flatten"
  top: "score"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
I1010 14:31:08.872413  5799 layer_factory.hpp:77] Creating layer data
I1010 14:31:08.872869  5799 net.cpp:100] Creating Layer data
I1010 14:31:08.872877  5799 net.cpp:408] data -> data
I1010 14:31:08.872906  5799 net.cpp:408] data -> label
I1010 14:31:08.872941  5799 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I1010 14:31:08.874258  5804 db_lmdb.cpp:35] Opened lmdb ../../../data/cifar-10/cifar10_train_lmdb
I1010 14:31:08.880290  5799 data_layer.cpp:41] output data size: 64,3,32,32
I1010 14:31:08.881906  5799 net.cpp:150] Setting up data
I1010 14:31:08.881924  5799 net.cpp:157] Top shape: 64 3 32 32 (196608)
I1010 14:31:08.881928  5799 net.cpp:157] Top shape: 64 (64)
I1010 14:31:08.881944  5799 net.cpp:165] Memory required for data: 786688
I1010 14:31:08.881953  5799 layer_factory.hpp:77] Creating layer label_data_1_split
I1010 14:31:08.881974  5799 net.cpp:100] Creating Layer label_data_1_split
I1010 14:31:08.881978  5799 net.cpp:434] label_data_1_split <- label
I1010 14:31:08.881990  5799 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1010 14:31:08.881997  5799 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1010 14:31:08.882041  5799 net.cpp:150] Setting up label_data_1_split
I1010 14:31:08.882061  5799 net.cpp:157] Top shape: 64 (64)
I1010 14:31:08.882091  5799 net.cpp:157] Top shape: 64 (64)
I1010 14:31:08.882098  5799 net.cpp:165] Memory required for data: 787200
I1010 14:31:08.882107  5799 layer_factory.hpp:77] Creating layer conv1
I1010 14:31:08.882141  5799 net.cpp:100] Creating Layer conv1
I1010 14:31:08.882151  5799 net.cpp:434] conv1 <- data
I1010 14:31:08.882163  5799 net.cpp:408] conv1 -> conv1
I1010 14:31:09.018673  5799 net.cpp:150] Setting up conv1
I1010 14:31:09.018698  5799 net.cpp:157] Top shape: 64 96 30 30 (5529600)
I1010 14:31:09.018702  5799 net.cpp:165] Memory required for data: 22905600
I1010 14:31:09.018718  5799 layer_factory.hpp:77] Creating layer relu1
I1010 14:31:09.018726  5799 net.cpp:100] Creating Layer relu1
I1010 14:31:09.018729  5799 net.cpp:434] relu1 <- conv1
I1010 14:31:09.018733  5799 net.cpp:395] relu1 -> conv1 (in-place)
I1010 14:31:09.018937  5799 net.cpp:150] Setting up relu1
I1010 14:31:09.018944  5799 net.cpp:157] Top shape: 64 96 30 30 (5529600)
I1010 14:31:09.018946  5799 net.cpp:165] Memory required for data: 45024000
I1010 14:31:09.018949  5799 layer_factory.hpp:77] Creating layer conv2
I1010 14:31:09.018956  5799 net.cpp:100] Creating Layer conv2
I1010 14:31:09.018959  5799 net.cpp:434] conv2 <- conv1
I1010 14:31:09.018961  5799 net.cpp:408] conv2 -> conv2
I1010 14:31:09.020341  5799 net.cpp:150] Setting up conv2
I1010 14:31:09.020352  5799 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1010 14:31:09.020354  5799 net.cpp:165] Memory required for data: 64291584
I1010 14:31:09.020360  5799 layer_factory.hpp:77] Creating layer bn1
I1010 14:31:09.020364  5799 net.cpp:100] Creating Layer bn1
I1010 14:31:09.020366  5799 net.cpp:434] bn1 <- conv2
I1010 14:31:09.020371  5799 net.cpp:408] bn1 -> bn1
I1010 14:31:09.020581  5799 net.cpp:150] Setting up bn1
I1010 14:31:09.020586  5799 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1010 14:31:09.020587  5799 net.cpp:165] Memory required for data: 83559168
I1010 14:31:09.020593  5799 layer_factory.hpp:77] Creating layer relu2
I1010 14:31:09.020597  5799 net.cpp:100] Creating Layer relu2
I1010 14:31:09.020599  5799 net.cpp:434] relu2 <- bn1
I1010 14:31:09.020602  5799 net.cpp:395] relu2 -> bn1 (in-place)
I1010 14:31:09.020774  5799 net.cpp:150] Setting up relu2
I1010 14:31:09.020779  5799 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1010 14:31:09.020792  5799 net.cpp:165] Memory required for data: 102826752
I1010 14:31:09.020794  5799 layer_factory.hpp:77] Creating layer drop1
I1010 14:31:09.020799  5799 net.cpp:100] Creating Layer drop1
I1010 14:31:09.020802  5799 net.cpp:434] drop1 <- bn1
I1010 14:31:09.020805  5799 net.cpp:408] drop1 -> drop1
I1010 14:31:09.020865  5799 net.cpp:150] Setting up drop1
I1010 14:31:09.020869  5799 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1010 14:31:09.020871  5799 net.cpp:165] Memory required for data: 122094336
I1010 14:31:09.020886  5799 layer_factory.hpp:77] Creating layer conv3
I1010 14:31:09.020892  5799 net.cpp:100] Creating Layer conv3
I1010 14:31:09.020895  5799 net.cpp:434] conv3 <- drop1
I1010 14:31:09.020915  5799 net.cpp:408] conv3 -> conv3
I1010 14:31:09.022126  5799 net.cpp:150] Setting up conv3
I1010 14:31:09.022135  5799 net.cpp:157] Top shape: 64 96 13 13 (1038336)
I1010 14:31:09.022137  5799 net.cpp:165] Memory required for data: 126247680
I1010 14:31:09.022143  5799 layer_factory.hpp:77] Creating layer relu3
I1010 14:31:09.022148  5799 net.cpp:100] Creating Layer relu3
I1010 14:31:09.022150  5799 net.cpp:434] relu3 <- conv3
I1010 14:31:09.022153  5799 net.cpp:395] relu3 -> conv3 (in-place)
I1010 14:31:09.022409  5799 net.cpp:150] Setting up relu3
I1010 14:31:09.022416  5799 net.cpp:157] Top shape: 64 96 13 13 (1038336)
I1010 14:31:09.022418  5799 net.cpp:165] Memory required for data: 130401024
I1010 14:31:09.022420  5799 layer_factory.hpp:77] Creating layer conv4
I1010 14:31:09.022426  5799 net.cpp:100] Creating Layer conv4
I1010 14:31:09.022429  5799 net.cpp:434] conv4 <- conv3
I1010 14:31:09.022433  5799 net.cpp:408] conv4 -> conv4
I1010 14:31:09.024255  5799 net.cpp:150] Setting up conv4
I1010 14:31:09.024265  5799 net.cpp:157] Top shape: 64 192 11 11 (1486848)
I1010 14:31:09.024266  5799 net.cpp:165] Memory required for data: 136348416
I1010 14:31:09.024271  5799 layer_factory.hpp:77] Creating layer relu4
I1010 14:31:09.024274  5799 net.cpp:100] Creating Layer relu4
I1010 14:31:09.024276  5799 net.cpp:434] relu4 <- conv4
I1010 14:31:09.024279  5799 net.cpp:395] relu4 -> conv4 (in-place)
I1010 14:31:09.024441  5799 net.cpp:150] Setting up relu4
I1010 14:31:09.024446  5799 net.cpp:157] Top shape: 64 192 11 11 (1486848)
I1010 14:31:09.024448  5799 net.cpp:165] Memory required for data: 142295808
I1010 14:31:09.024451  5799 layer_factory.hpp:77] Creating layer conv5
I1010 14:31:09.024456  5799 net.cpp:100] Creating Layer conv5
I1010 14:31:09.024458  5799 net.cpp:434] conv5 <- conv4
I1010 14:31:09.024462  5799 net.cpp:408] conv5 -> conv5
I1010 14:31:09.026983  5799 net.cpp:150] Setting up conv5
I1010 14:31:09.026993  5799 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1010 14:31:09.026995  5799 net.cpp:165] Memory required for data: 146277120
I1010 14:31:09.026999  5799 layer_factory.hpp:77] Creating layer bn2
I1010 14:31:09.027004  5799 net.cpp:100] Creating Layer bn2
I1010 14:31:09.027006  5799 net.cpp:434] bn2 <- conv5
I1010 14:31:09.027010  5799 net.cpp:408] bn2 -> bn2
I1010 14:31:09.027192  5799 net.cpp:150] Setting up bn2
I1010 14:31:09.027197  5799 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1010 14:31:09.027199  5799 net.cpp:165] Memory required for data: 150258432
I1010 14:31:09.027204  5799 layer_factory.hpp:77] Creating layer relu5
I1010 14:31:09.027207  5799 net.cpp:100] Creating Layer relu5
I1010 14:31:09.027209  5799 net.cpp:434] relu5 <- bn2
I1010 14:31:09.027211  5799 net.cpp:395] relu5 -> bn2 (in-place)
I1010 14:31:09.027369  5799 net.cpp:150] Setting up relu5
I1010 14:31:09.027375  5799 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1010 14:31:09.027377  5799 net.cpp:165] Memory required for data: 154239744
I1010 14:31:09.027379  5799 layer_factory.hpp:77] Creating layer drop4
I1010 14:31:09.027384  5799 net.cpp:100] Creating Layer drop4
I1010 14:31:09.027385  5799 net.cpp:434] drop4 <- bn2
I1010 14:31:09.027389  5799 net.cpp:408] drop4 -> drop4
I1010 14:31:09.027444  5799 net.cpp:150] Setting up drop4
I1010 14:31:09.027449  5799 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1010 14:31:09.027475  5799 net.cpp:165] Memory required for data: 158221056
I1010 14:31:09.027493  5799 layer_factory.hpp:77] Creating layer conv6
I1010 14:31:09.027501  5799 net.cpp:100] Creating Layer conv6
I1010 14:31:09.027515  5799 net.cpp:434] conv6 <- drop4
I1010 14:31:09.027519  5799 net.cpp:408] conv6 -> conv6
I1010 14:31:09.030052  5799 net.cpp:150] Setting up conv6
I1010 14:31:09.030078  5799 net.cpp:157] Top shape: 64 192 4 4 (196608)
I1010 14:31:09.030081  5799 net.cpp:165] Memory required for data: 159007488
I1010 14:31:09.030092  5799 layer_factory.hpp:77] Creating layer relu6
I1010 14:31:09.030097  5799 net.cpp:100] Creating Layer relu6
I1010 14:31:09.030099  5799 net.cpp:434] relu6 <- conv6
I1010 14:31:09.030117  5799 net.cpp:395] relu6 -> conv6 (in-place)
I1010 14:31:09.030376  5799 net.cpp:150] Setting up relu6
I1010 14:31:09.030400  5799 net.cpp:157] Top shape: 64 192 4 4 (196608)
I1010 14:31:09.030401  5799 net.cpp:165] Memory required for data: 159793920
I1010 14:31:09.030403  5799 layer_factory.hpp:77] Creating layer conv7
I1010 14:31:09.030411  5799 net.cpp:100] Creating Layer conv7
I1010 14:31:09.030413  5799 net.cpp:434] conv7 <- conv6
I1010 14:31:09.030417  5799 net.cpp:408] conv7 -> conv7
I1010 14:31:09.033555  5799 net.cpp:150] Setting up conv7
I1010 14:31:09.033571  5799 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1010 14:31:09.033574  5799 net.cpp:165] Memory required for data: 159990528
I1010 14:31:09.033581  5799 layer_factory.hpp:77] Creating layer relu7
I1010 14:31:09.033586  5799 net.cpp:100] Creating Layer relu7
I1010 14:31:09.033588  5799 net.cpp:434] relu7 <- conv7
I1010 14:31:09.033592  5799 net.cpp:395] relu7 -> conv7 (in-place)
I1010 14:31:09.033849  5799 net.cpp:150] Setting up relu7
I1010 14:31:09.033855  5799 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1010 14:31:09.033857  5799 net.cpp:165] Memory required for data: 160187136
I1010 14:31:09.033859  5799 layer_factory.hpp:77] Creating layer conv8
I1010 14:31:09.033867  5799 net.cpp:100] Creating Layer conv8
I1010 14:31:09.033869  5799 net.cpp:434] conv8 <- conv7
I1010 14:31:09.033872  5799 net.cpp:408] conv8 -> conv8
I1010 14:31:09.035125  5799 net.cpp:150] Setting up conv8
I1010 14:31:09.035135  5799 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1010 14:31:09.035136  5799 net.cpp:165] Memory required for data: 160383744
I1010 14:31:09.035141  5799 layer_factory.hpp:77] Creating layer relu8
I1010 14:31:09.035145  5799 net.cpp:100] Creating Layer relu8
I1010 14:31:09.035147  5799 net.cpp:434] relu8 <- conv8
I1010 14:31:09.035151  5799 net.cpp:395] relu8 -> conv8 (in-place)
I1010 14:31:09.035320  5799 net.cpp:150] Setting up relu8
I1010 14:31:09.035326  5799 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1010 14:31:09.035327  5799 net.cpp:165] Memory required for data: 160580352
I1010 14:31:09.035329  5799 layer_factory.hpp:77] Creating layer conv9
I1010 14:31:09.035336  5799 net.cpp:100] Creating Layer conv9
I1010 14:31:09.035337  5799 net.cpp:434] conv9 <- conv8
I1010 14:31:09.035341  5799 net.cpp:408] conv9 -> conv9
I1010 14:31:09.036166  5799 net.cpp:150] Setting up conv9
I1010 14:31:09.036175  5799 net.cpp:157] Top shape: 64 10 2 2 (2560)
I1010 14:31:09.036178  5799 net.cpp:165] Memory required for data: 160590592
I1010 14:31:09.036182  5799 layer_factory.hpp:77] Creating layer relu9
I1010 14:31:09.036185  5799 net.cpp:100] Creating Layer relu9
I1010 14:31:09.036187  5799 net.cpp:434] relu9 <- conv9
I1010 14:31:09.036192  5799 net.cpp:395] relu9 -> conv9 (in-place)
I1010 14:31:09.036445  5799 net.cpp:150] Setting up relu9
I1010 14:31:09.036453  5799 net.cpp:157] Top shape: 64 10 2 2 (2560)
I1010 14:31:09.036454  5799 net.cpp:165] Memory required for data: 160600832
I1010 14:31:09.036456  5799 layer_factory.hpp:77] Creating layer pool
I1010 14:31:09.036461  5799 net.cpp:100] Creating Layer pool
I1010 14:31:09.036463  5799 net.cpp:434] pool <- conv9
I1010 14:31:09.036468  5799 net.cpp:408] pool -> pool
I1010 14:31:09.036659  5799 net.cpp:150] Setting up pool
I1010 14:31:09.036664  5799 net.cpp:157] Top shape: 64 10 1 1 (640)
I1010 14:31:09.036676  5799 net.cpp:165] Memory required for data: 160603392
I1010 14:31:09.036679  5799 layer_factory.hpp:77] Creating layer flatten
I1010 14:31:09.036682  5799 net.cpp:100] Creating Layer flatten
I1010 14:31:09.036684  5799 net.cpp:434] flatten <- pool
I1010 14:31:09.036689  5799 net.cpp:408] flatten -> flatten
I1010 14:31:09.036723  5799 net.cpp:150] Setting up flatten
I1010 14:31:09.036727  5799 net.cpp:157] Top shape: 64 10 (640)
I1010 14:31:09.036730  5799 net.cpp:165] Memory required for data: 160605952
I1010 14:31:09.036731  5799 layer_factory.hpp:77] Creating layer score
I1010 14:31:09.036749  5799 net.cpp:100] Creating Layer score
I1010 14:31:09.036752  5799 net.cpp:434] score <- flatten
I1010 14:31:09.036770  5799 net.cpp:408] score -> score
I1010 14:31:09.036906  5799 net.cpp:150] Setting up score
I1010 14:31:09.036909  5799 net.cpp:157] Top shape: 64 10 (640)
I1010 14:31:09.036911  5799 net.cpp:165] Memory required for data: 160608512
I1010 14:31:09.036916  5799 layer_factory.hpp:77] Creating layer score_score_0_split
I1010 14:31:09.036919  5799 net.cpp:100] Creating Layer score_score_0_split
I1010 14:31:09.036921  5799 net.cpp:434] score_score_0_split <- score
I1010 14:31:09.036926  5799 net.cpp:408] score_score_0_split -> score_score_0_split_0
I1010 14:31:09.036936  5799 net.cpp:408] score_score_0_split -> score_score_0_split_1
I1010 14:31:09.036978  5799 net.cpp:150] Setting up score_score_0_split
I1010 14:31:09.036995  5799 net.cpp:157] Top shape: 64 10 (640)
I1010 14:31:09.036998  5799 net.cpp:157] Top shape: 64 10 (640)
I1010 14:31:09.037000  5799 net.cpp:165] Memory required for data: 160613632
I1010 14:31:09.037015  5799 layer_factory.hpp:77] Creating layer accuracy
I1010 14:31:09.037019  5799 net.cpp:100] Creating Layer accuracy
I1010 14:31:09.037021  5799 net.cpp:434] accuracy <- score_score_0_split_0
I1010 14:31:09.037024  5799 net.cpp:434] accuracy <- label_data_1_split_0
I1010 14:31:09.037044  5799 net.cpp:408] accuracy -> accuracy
I1010 14:31:09.037048  5799 net.cpp:150] Setting up accuracy
I1010 14:31:09.037065  5799 net.cpp:157] Top shape: (1)
I1010 14:31:09.037067  5799 net.cpp:165] Memory required for data: 160613636
I1010 14:31:09.037070  5799 layer_factory.hpp:77] Creating layer loss
I1010 14:31:09.037087  5799 net.cpp:100] Creating Layer loss
I1010 14:31:09.037091  5799 net.cpp:434] loss <- score_score_0_split_1
I1010 14:31:09.037093  5799 net.cpp:434] loss <- label_data_1_split_1
I1010 14:31:09.037098  5799 net.cpp:408] loss -> loss
I1010 14:31:09.037103  5799 layer_factory.hpp:77] Creating layer loss
I1010 14:31:09.037438  5799 net.cpp:150] Setting up loss
I1010 14:31:09.037446  5799 net.cpp:157] Top shape: (1)
I1010 14:31:09.037447  5799 net.cpp:160]     with loss weight 1
I1010 14:31:09.037459  5799 net.cpp:165] Memory required for data: 160613640
I1010 14:31:09.037461  5799 net.cpp:226] loss needs backward computation.
I1010 14:31:09.037467  5799 net.cpp:228] accuracy does not need backward computation.
I1010 14:31:09.037469  5799 net.cpp:226] score_score_0_split needs backward computation.
I1010 14:31:09.037472  5799 net.cpp:226] score needs backward computation.
I1010 14:31:09.037474  5799 net.cpp:226] flatten needs backward computation.
I1010 14:31:09.037475  5799 net.cpp:226] pool needs backward computation.
I1010 14:31:09.037477  5799 net.cpp:226] relu9 needs backward computation.
I1010 14:31:09.037494  5799 net.cpp:226] conv9 needs backward computation.
I1010 14:31:09.037495  5799 net.cpp:226] relu8 needs backward computation.
I1010 14:31:09.037497  5799 net.cpp:226] conv8 needs backward computation.
I1010 14:31:09.037499  5799 net.cpp:226] relu7 needs backward computation.
I1010 14:31:09.037502  5799 net.cpp:226] conv7 needs backward computation.
I1010 14:31:09.037503  5799 net.cpp:226] relu6 needs backward computation.
I1010 14:31:09.037505  5799 net.cpp:226] conv6 needs backward computation.
I1010 14:31:09.037508  5799 net.cpp:226] drop4 needs backward computation.
I1010 14:31:09.037510  5799 net.cpp:226] relu5 needs backward computation.
I1010 14:31:09.037513  5799 net.cpp:226] bn2 needs backward computation.
I1010 14:31:09.037539  5799 net.cpp:226] conv5 needs backward computation.
I1010 14:31:09.037540  5799 net.cpp:226] relu4 needs backward computation.
I1010 14:31:09.037555  5799 net.cpp:226] conv4 needs backward computation.
I1010 14:31:09.037557  5799 net.cpp:226] relu3 needs backward computation.
I1010 14:31:09.037559  5799 net.cpp:226] conv3 needs backward computation.
I1010 14:31:09.037561  5799 net.cpp:226] drop1 needs backward computation.
I1010 14:31:09.037564  5799 net.cpp:226] relu2 needs backward computation.
I1010 14:31:09.037581  5799 net.cpp:226] bn1 needs backward computation.
I1010 14:31:09.037583  5799 net.cpp:226] conv2 needs backward computation.
I1010 14:31:09.037585  5799 net.cpp:226] relu1 needs backward computation.
I1010 14:31:09.037600  5799 net.cpp:226] conv1 needs backward computation.
I1010 14:31:09.037603  5799 net.cpp:228] label_data_1_split does not need backward computation.
I1010 14:31:09.037608  5799 net.cpp:228] data does not need backward computation.
I1010 14:31:09.037611  5799 net.cpp:270] This network produces output accuracy
I1010 14:31:09.037613  5799 net.cpp:270] This network produces output loss
I1010 14:31:09.037629  5799 net.cpp:283] Network initialization done.
I1010 14:31:09.037847  5799 solver.cpp:181] Creating test net (#0) specified by test_net file: /home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_test.prototxt
I1010 14:31:09.038017  5799 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "../../../data/cifar-10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn1"
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "bn1"
  top: "bn1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "bn1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv5"
  top: "bn2"
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "bn2"
  top: "bn2"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "bn2"
  top: "drop4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "drop4"
  top: "conv6"
  convolution_param {
    num_output: 192
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "conv6"
  top: "conv6"
}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "conv6"
  top: "conv7"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "conv7"
  top: "conv8"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "conv8"
  top: "conv9"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}
layer {
  name: "pool"
  type: "Pooling"
  bottom: "conv9"
  top: "pool"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "flatten"
  type: "Flatten"
  bottom: "pool"
  top: "flatten"
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "flatten"
  top: "score"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
I1010 14:31:09.038148  5799 layer_factory.hpp:77] Creating layer data
I1010 14:31:09.038229  5799 net.cpp:100] Creating Layer data
I1010 14:31:09.038236  5799 net.cpp:408] data -> data
I1010 14:31:09.038244  5799 net.cpp:408] data -> label
I1010 14:31:09.038249  5799 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I1010 14:31:09.039057  5806 db_lmdb.cpp:35] Opened lmdb ../../../data/cifar-10/cifar10_test_lmdb
I1010 14:31:09.039162  5799 data_layer.cpp:41] output data size: 100,3,32,32
I1010 14:31:09.041213  5799 net.cpp:150] Setting up data
I1010 14:31:09.041249  5799 net.cpp:157] Top shape: 100 3 32 32 (307200)
I1010 14:31:09.041252  5799 net.cpp:157] Top shape: 100 (100)
I1010 14:31:09.041254  5799 net.cpp:165] Memory required for data: 1229200
I1010 14:31:09.041259  5799 layer_factory.hpp:77] Creating layer label_data_1_split
I1010 14:31:09.041281  5799 net.cpp:100] Creating Layer label_data_1_split
I1010 14:31:09.041285  5799 net.cpp:434] label_data_1_split <- label
I1010 14:31:09.041304  5799 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1010 14:31:09.041312  5799 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1010 14:31:09.041380  5799 net.cpp:150] Setting up label_data_1_split
I1010 14:31:09.041386  5799 net.cpp:157] Top shape: 100 (100)
I1010 14:31:09.041388  5799 net.cpp:157] Top shape: 100 (100)
I1010 14:31:09.041390  5799 net.cpp:165] Memory required for data: 1230000
I1010 14:31:09.041393  5799 layer_factory.hpp:77] Creating layer conv1
I1010 14:31:09.041401  5799 net.cpp:100] Creating Layer conv1
I1010 14:31:09.041404  5799 net.cpp:434] conv1 <- data
I1010 14:31:09.041407  5799 net.cpp:408] conv1 -> conv1
I1010 14:31:09.042500  5799 net.cpp:150] Setting up conv1
I1010 14:31:09.042526  5799 net.cpp:157] Top shape: 100 96 30 30 (8640000)
I1010 14:31:09.042528  5799 net.cpp:165] Memory required for data: 35790000
I1010 14:31:09.042536  5799 layer_factory.hpp:77] Creating layer relu1
I1010 14:31:09.042541  5799 net.cpp:100] Creating Layer relu1
I1010 14:31:09.042546  5799 net.cpp:434] relu1 <- conv1
I1010 14:31:09.042549  5799 net.cpp:395] relu1 -> conv1 (in-place)
I1010 14:31:09.042717  5799 net.cpp:150] Setting up relu1
I1010 14:31:09.042723  5799 net.cpp:157] Top shape: 100 96 30 30 (8640000)
I1010 14:31:09.042742  5799 net.cpp:165] Memory required for data: 70350000
I1010 14:31:09.042743  5799 layer_factory.hpp:77] Creating layer conv2
I1010 14:31:09.042750  5799 net.cpp:100] Creating Layer conv2
I1010 14:31:09.042753  5799 net.cpp:434] conv2 <- conv1
I1010 14:31:09.042758  5799 net.cpp:408] conv2 -> conv2
I1010 14:31:09.044140  5799 net.cpp:150] Setting up conv2
I1010 14:31:09.044165  5799 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1010 14:31:09.044168  5799 net.cpp:165] Memory required for data: 100455600
I1010 14:31:09.044174  5799 layer_factory.hpp:77] Creating layer bn1
I1010 14:31:09.044180  5799 net.cpp:100] Creating Layer bn1
I1010 14:31:09.044183  5799 net.cpp:434] bn1 <- conv2
I1010 14:31:09.044198  5799 net.cpp:408] bn1 -> bn1
I1010 14:31:09.044400  5799 net.cpp:150] Setting up bn1
I1010 14:31:09.044411  5799 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1010 14:31:09.044428  5799 net.cpp:165] Memory required for data: 130561200
I1010 14:31:09.044435  5799 layer_factory.hpp:77] Creating layer relu2
I1010 14:31:09.044456  5799 net.cpp:100] Creating Layer relu2
I1010 14:31:09.044457  5799 net.cpp:434] relu2 <- bn1
I1010 14:31:09.044461  5799 net.cpp:395] relu2 -> bn1 (in-place)
I1010 14:31:09.044724  5799 net.cpp:150] Setting up relu2
I1010 14:31:09.044746  5799 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1010 14:31:09.044749  5799 net.cpp:165] Memory required for data: 160666800
I1010 14:31:09.044751  5799 layer_factory.hpp:77] Creating layer drop1
I1010 14:31:09.044756  5799 net.cpp:100] Creating Layer drop1
I1010 14:31:09.044759  5799 net.cpp:434] drop1 <- bn1
I1010 14:31:09.044762  5799 net.cpp:408] drop1 -> drop1
I1010 14:31:09.044814  5799 net.cpp:150] Setting up drop1
I1010 14:31:09.044819  5799 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1010 14:31:09.044836  5799 net.cpp:165] Memory required for data: 190772400
I1010 14:31:09.044838  5799 layer_factory.hpp:77] Creating layer conv3
I1010 14:31:09.044847  5799 net.cpp:100] Creating Layer conv3
I1010 14:31:09.044849  5799 net.cpp:434] conv3 <- drop1
I1010 14:31:09.044852  5799 net.cpp:408] conv3 -> conv3
I1010 14:31:09.046231  5799 net.cpp:150] Setting up conv3
I1010 14:31:09.046241  5799 net.cpp:157] Top shape: 100 96 13 13 (1622400)
I1010 14:31:09.046243  5799 net.cpp:165] Memory required for data: 197262000
I1010 14:31:09.046250  5799 layer_factory.hpp:77] Creating layer relu3
I1010 14:31:09.046257  5799 net.cpp:100] Creating Layer relu3
I1010 14:31:09.046259  5799 net.cpp:434] relu3 <- conv3
I1010 14:31:09.046264  5799 net.cpp:395] relu3 -> conv3 (in-place)
I1010 14:31:09.046494  5799 net.cpp:150] Setting up relu3
I1010 14:31:09.046502  5799 net.cpp:157] Top shape: 100 96 13 13 (1622400)
I1010 14:31:09.046505  5799 net.cpp:165] Memory required for data: 203751600
I1010 14:31:09.046507  5799 layer_factory.hpp:77] Creating layer conv4
I1010 14:31:09.046515  5799 net.cpp:100] Creating Layer conv4
I1010 14:31:09.046519  5799 net.cpp:434] conv4 <- conv3
I1010 14:31:09.046522  5799 net.cpp:408] conv4 -> conv4
I1010 14:31:09.048638  5799 net.cpp:150] Setting up conv4
I1010 14:31:09.048655  5799 net.cpp:157] Top shape: 100 192 11 11 (2323200)
I1010 14:31:09.048656  5799 net.cpp:165] Memory required for data: 213044400
I1010 14:31:09.048662  5799 layer_factory.hpp:77] Creating layer relu4
I1010 14:31:09.048681  5799 net.cpp:100] Creating Layer relu4
I1010 14:31:09.048684  5799 net.cpp:434] relu4 <- conv4
I1010 14:31:09.048687  5799 net.cpp:395] relu4 -> conv4 (in-place)
I1010 14:31:09.048882  5799 net.cpp:150] Setting up relu4
I1010 14:31:09.048888  5799 net.cpp:157] Top shape: 100 192 11 11 (2323200)
I1010 14:31:09.048890  5799 net.cpp:165] Memory required for data: 222337200
I1010 14:31:09.048892  5799 layer_factory.hpp:77] Creating layer conv5
I1010 14:31:09.048898  5799 net.cpp:100] Creating Layer conv5
I1010 14:31:09.048913  5799 net.cpp:434] conv5 <- conv4
I1010 14:31:09.048918  5799 net.cpp:408] conv5 -> conv5
I1010 14:31:09.051622  5799 net.cpp:150] Setting up conv5
I1010 14:31:09.051633  5799 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1010 14:31:09.051636  5799 net.cpp:165] Memory required for data: 228558000
I1010 14:31:09.051640  5799 layer_factory.hpp:77] Creating layer bn2
I1010 14:31:09.051645  5799 net.cpp:100] Creating Layer bn2
I1010 14:31:09.051646  5799 net.cpp:434] bn2 <- conv5
I1010 14:31:09.051651  5799 net.cpp:408] bn2 -> bn2
I1010 14:31:09.051864  5799 net.cpp:150] Setting up bn2
I1010 14:31:09.051869  5799 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1010 14:31:09.051872  5799 net.cpp:165] Memory required for data: 234778800
I1010 14:31:09.051875  5799 layer_factory.hpp:77] Creating layer relu5
I1010 14:31:09.051879  5799 net.cpp:100] Creating Layer relu5
I1010 14:31:09.051882  5799 net.cpp:434] relu5 <- bn2
I1010 14:31:09.051894  5799 net.cpp:395] relu5 -> bn2 (in-place)
I1010 14:31:09.052171  5799 net.cpp:150] Setting up relu5
I1010 14:31:09.052180  5799 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1010 14:31:09.052181  5799 net.cpp:165] Memory required for data: 240999600
I1010 14:31:09.052183  5799 layer_factory.hpp:77] Creating layer drop4
I1010 14:31:09.052187  5799 net.cpp:100] Creating Layer drop4
I1010 14:31:09.052189  5799 net.cpp:434] drop4 <- bn2
I1010 14:31:09.052193  5799 net.cpp:408] drop4 -> drop4
I1010 14:31:09.052255  5799 net.cpp:150] Setting up drop4
I1010 14:31:09.052273  5799 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1010 14:31:09.052275  5799 net.cpp:165] Memory required for data: 247220400
I1010 14:31:09.052278  5799 layer_factory.hpp:77] Creating layer conv6
I1010 14:31:09.052300  5799 net.cpp:100] Creating Layer conv6
I1010 14:31:09.052302  5799 net.cpp:434] conv6 <- drop4
I1010 14:31:09.052305  5799 net.cpp:408] conv6 -> conv6
I1010 14:31:09.054874  5799 net.cpp:150] Setting up conv6
I1010 14:31:09.054884  5799 net.cpp:157] Top shape: 100 192 4 4 (307200)
I1010 14:31:09.054888  5799 net.cpp:165] Memory required for data: 248449200
I1010 14:31:09.054895  5799 layer_factory.hpp:77] Creating layer relu6
I1010 14:31:09.054900  5799 net.cpp:100] Creating Layer relu6
I1010 14:31:09.054903  5799 net.cpp:434] relu6 <- conv6
I1010 14:31:09.054905  5799 net.cpp:395] relu6 -> conv6 (in-place)
I1010 14:31:09.055163  5799 net.cpp:150] Setting up relu6
I1010 14:31:09.055172  5799 net.cpp:157] Top shape: 100 192 4 4 (307200)
I1010 14:31:09.055174  5799 net.cpp:165] Memory required for data: 249678000
I1010 14:31:09.055176  5799 layer_factory.hpp:77] Creating layer conv7
I1010 14:31:09.055181  5799 net.cpp:100] Creating Layer conv7
I1010 14:31:09.055183  5799 net.cpp:434] conv7 <- conv6
I1010 14:31:09.055188  5799 net.cpp:408] conv7 -> conv7
I1010 14:31:09.057807  5799 net.cpp:150] Setting up conv7
I1010 14:31:09.057818  5799 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1010 14:31:09.057821  5799 net.cpp:165] Memory required for data: 249985200
I1010 14:31:09.057824  5799 layer_factory.hpp:77] Creating layer relu7
I1010 14:31:09.057828  5799 net.cpp:100] Creating Layer relu7
I1010 14:31:09.057831  5799 net.cpp:434] relu7 <- conv7
I1010 14:31:09.057833  5799 net.cpp:395] relu7 -> conv7 (in-place)
I1010 14:31:09.058007  5799 net.cpp:150] Setting up relu7
I1010 14:31:09.058012  5799 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1010 14:31:09.058014  5799 net.cpp:165] Memory required for data: 250292400
I1010 14:31:09.058017  5799 layer_factory.hpp:77] Creating layer conv8
I1010 14:31:09.058022  5799 net.cpp:100] Creating Layer conv8
I1010 14:31:09.058024  5799 net.cpp:434] conv8 <- conv7
I1010 14:31:09.058028  5799 net.cpp:408] conv8 -> conv8
I1010 14:31:09.059015  5799 net.cpp:150] Setting up conv8
I1010 14:31:09.059025  5799 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1010 14:31:09.059026  5799 net.cpp:165] Memory required for data: 250599600
I1010 14:31:09.059031  5799 layer_factory.hpp:77] Creating layer relu8
I1010 14:31:09.059033  5799 net.cpp:100] Creating Layer relu8
I1010 14:31:09.059036  5799 net.cpp:434] relu8 <- conv8
I1010 14:31:09.059038  5799 net.cpp:395] relu8 -> conv8 (in-place)
I1010 14:31:09.059298  5799 net.cpp:150] Setting up relu8
I1010 14:31:09.059306  5799 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1010 14:31:09.059309  5799 net.cpp:165] Memory required for data: 250906800
I1010 14:31:09.059310  5799 layer_factory.hpp:77] Creating layer conv9
I1010 14:31:09.059316  5799 net.cpp:100] Creating Layer conv9
I1010 14:31:09.059319  5799 net.cpp:434] conv9 <- conv8
I1010 14:31:09.059322  5799 net.cpp:408] conv9 -> conv9
I1010 14:31:09.060082  5799 net.cpp:150] Setting up conv9
I1010 14:31:09.060091  5799 net.cpp:157] Top shape: 100 10 2 2 (4000)
I1010 14:31:09.060093  5799 net.cpp:165] Memory required for data: 250922800
I1010 14:31:09.060097  5799 layer_factory.hpp:77] Creating layer relu9
I1010 14:31:09.060101  5799 net.cpp:100] Creating Layer relu9
I1010 14:31:09.060103  5799 net.cpp:434] relu9 <- conv9
I1010 14:31:09.060117  5799 net.cpp:395] relu9 -> conv9 (in-place)
I1010 14:31:09.060403  5799 net.cpp:150] Setting up relu9
I1010 14:31:09.060411  5799 net.cpp:157] Top shape: 100 10 2 2 (4000)
I1010 14:31:09.060413  5799 net.cpp:165] Memory required for data: 250938800
I1010 14:31:09.060415  5799 layer_factory.hpp:77] Creating layer pool
I1010 14:31:09.060420  5799 net.cpp:100] Creating Layer pool
I1010 14:31:09.060421  5799 net.cpp:434] pool <- conv9
I1010 14:31:09.060425  5799 net.cpp:408] pool -> pool
I1010 14:31:09.060611  5799 net.cpp:150] Setting up pool
I1010 14:31:09.060616  5799 net.cpp:157] Top shape: 100 10 1 1 (1000)
I1010 14:31:09.060618  5799 net.cpp:165] Memory required for data: 250942800
I1010 14:31:09.060621  5799 layer_factory.hpp:77] Creating layer flatten
I1010 14:31:09.060626  5799 net.cpp:100] Creating Layer flatten
I1010 14:31:09.060627  5799 net.cpp:434] flatten <- pool
I1010 14:31:09.060631  5799 net.cpp:408] flatten -> flatten
I1010 14:31:09.060665  5799 net.cpp:150] Setting up flatten
I1010 14:31:09.060670  5799 net.cpp:157] Top shape: 100 10 (1000)
I1010 14:31:09.060672  5799 net.cpp:165] Memory required for data: 250946800
I1010 14:31:09.060673  5799 layer_factory.hpp:77] Creating layer score
I1010 14:31:09.060678  5799 net.cpp:100] Creating Layer score
I1010 14:31:09.060680  5799 net.cpp:434] score <- flatten
I1010 14:31:09.060698  5799 net.cpp:408] score -> score
I1010 14:31:09.060835  5799 net.cpp:150] Setting up score
I1010 14:31:09.060840  5799 net.cpp:157] Top shape: 100 10 (1000)
I1010 14:31:09.060842  5799 net.cpp:165] Memory required for data: 250950800
I1010 14:31:09.060847  5799 layer_factory.hpp:77] Creating layer score_score_0_split
I1010 14:31:09.060852  5799 net.cpp:100] Creating Layer score_score_0_split
I1010 14:31:09.060854  5799 net.cpp:434] score_score_0_split <- score
I1010 14:31:09.060873  5799 net.cpp:408] score_score_0_split -> score_score_0_split_0
I1010 14:31:09.060878  5799 net.cpp:408] score_score_0_split -> score_score_0_split_1
I1010 14:31:09.060940  5799 net.cpp:150] Setting up score_score_0_split
I1010 14:31:09.060945  5799 net.cpp:157] Top shape: 100 10 (1000)
I1010 14:31:09.060947  5799 net.cpp:157] Top shape: 100 10 (1000)
I1010 14:31:09.060950  5799 net.cpp:165] Memory required for data: 250958800
I1010 14:31:09.060951  5799 layer_factory.hpp:77] Creating layer accuracy
I1010 14:31:09.060956  5799 net.cpp:100] Creating Layer accuracy
I1010 14:31:09.060957  5799 net.cpp:434] accuracy <- score_score_0_split_0
I1010 14:31:09.060961  5799 net.cpp:434] accuracy <- label_data_1_split_0
I1010 14:31:09.060964  5799 net.cpp:408] accuracy -> accuracy
I1010 14:31:09.060971  5799 net.cpp:150] Setting up accuracy
I1010 14:31:09.060972  5799 net.cpp:157] Top shape: (1)
I1010 14:31:09.060974  5799 net.cpp:165] Memory required for data: 250958804
I1010 14:31:09.060976  5799 layer_factory.hpp:77] Creating layer loss
I1010 14:31:09.060979  5799 net.cpp:100] Creating Layer loss
I1010 14:31:09.060981  5799 net.cpp:434] loss <- score_score_0_split_1
I1010 14:31:09.060984  5799 net.cpp:434] loss <- label_data_1_split_1
I1010 14:31:09.060987  5799 net.cpp:408] loss -> loss
I1010 14:31:09.060992  5799 layer_factory.hpp:77] Creating layer loss
I1010 14:31:09.061352  5799 net.cpp:150] Setting up loss
I1010 14:31:09.061360  5799 net.cpp:157] Top shape: (1)
I1010 14:31:09.061363  5799 net.cpp:160]     with loss weight 1
I1010 14:31:09.061386  5799 net.cpp:165] Memory required for data: 250958808
I1010 14:31:09.061388  5799 net.cpp:226] loss needs backward computation.
I1010 14:31:09.061393  5799 net.cpp:228] accuracy does not need backward computation.
I1010 14:31:09.061395  5799 net.cpp:226] score_score_0_split needs backward computation.
I1010 14:31:09.061398  5799 net.cpp:226] score needs backward computation.
I1010 14:31:09.061399  5799 net.cpp:226] flatten needs backward computation.
I1010 14:31:09.061401  5799 net.cpp:226] pool needs backward computation.
I1010 14:31:09.061403  5799 net.cpp:226] relu9 needs backward computation.
I1010 14:31:09.061405  5799 net.cpp:226] conv9 needs backward computation.
I1010 14:31:09.061415  5799 net.cpp:226] relu8 needs backward computation.
I1010 14:31:09.061417  5799 net.cpp:226] conv8 needs backward computation.
I1010 14:31:09.061419  5799 net.cpp:226] relu7 needs backward computation.
I1010 14:31:09.061421  5799 net.cpp:226] conv7 needs backward computation.
I1010 14:31:09.061424  5799 net.cpp:226] relu6 needs backward computation.
I1010 14:31:09.061439  5799 net.cpp:226] conv6 needs backward computation.
I1010 14:31:09.061440  5799 net.cpp:226] drop4 needs backward computation.
I1010 14:31:09.061444  5799 net.cpp:226] relu5 needs backward computation.
I1010 14:31:09.061445  5799 net.cpp:226] bn2 needs backward computation.
I1010 14:31:09.061447  5799 net.cpp:226] conv5 needs backward computation.
I1010 14:31:09.061450  5799 net.cpp:226] relu4 needs backward computation.
I1010 14:31:09.061452  5799 net.cpp:226] conv4 needs backward computation.
I1010 14:31:09.061455  5799 net.cpp:226] relu3 needs backward computation.
I1010 14:31:09.061456  5799 net.cpp:226] conv3 needs backward computation.
I1010 14:31:09.061458  5799 net.cpp:226] drop1 needs backward computation.
I1010 14:31:09.061460  5799 net.cpp:226] relu2 needs backward computation.
I1010 14:31:09.061463  5799 net.cpp:226] bn1 needs backward computation.
I1010 14:31:09.061465  5799 net.cpp:226] conv2 needs backward computation.
I1010 14:31:09.061470  5799 net.cpp:226] relu1 needs backward computation.
I1010 14:31:09.061471  5799 net.cpp:226] conv1 needs backward computation.
I1010 14:31:09.061475  5799 net.cpp:228] label_data_1_split does not need backward computation.
I1010 14:31:09.061476  5799 net.cpp:228] data does not need backward computation.
I1010 14:31:09.061478  5799 net.cpp:270] This network produces output accuracy
I1010 14:31:09.061480  5799 net.cpp:270] This network produces output loss
I1010 14:31:09.061496  5799 net.cpp:283] Network initialization done.
I1010 14:31:09.061545  5799 solver.cpp:60] Solver scaffolding done.
I1010 14:31:09.062620  5799 caffe.cpp:251] Starting Optimization
I1010 14:31:09.062625  5799 solver.cpp:279] Solving 
I1010 14:31:09.062628  5799 solver.cpp:280] Learning Rate Policy: step
I1010 14:31:09.063558  5799 solver.cpp:337] Iteration 0, Testing net (#0)
I1010 14:31:12.580426  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1056
I1010 14:31:12.580449  5799 solver.cpp:404]     Test net output #1: loss = 78.1054 (* 1 = 78.1054 loss)
I1010 14:31:12.618661  5799 solver.cpp:228] Iteration 0, loss = 2.50131
I1010 14:31:12.618683  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:31:12.618691  5799 solver.cpp:244]     Train net output #1: loss = 2.50131 (* 1 = 2.50131 loss)
I1010 14:31:12.618705  5799 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I1010 14:31:16.803148  5799 solver.cpp:228] Iteration 50, loss = 2.46969
I1010 14:31:16.803184  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:31:16.803191  5799 solver.cpp:244]     Train net output #1: loss = 2.46969 (* 1 = 2.46969 loss)
I1010 14:31:16.803196  5799 sgd_solver.cpp:106] Iteration 50, lr = 0.0001
I1010 14:31:20.959416  5799 solver.cpp:228] Iteration 100, loss = 2.63777
I1010 14:31:20.959435  5799 solver.cpp:244]     Train net output #0: accuracy = 0.0625
I1010 14:31:20.959444  5799 solver.cpp:244]     Train net output #1: loss = 2.63777 (* 1 = 2.63777 loss)
I1010 14:31:20.959446  5799 sgd_solver.cpp:106] Iteration 100, lr = 0.0001
I1010 14:31:25.114552  5799 solver.cpp:228] Iteration 150, loss = 2.65979
I1010 14:31:25.114572  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:31:25.114579  5799 solver.cpp:244]     Train net output #1: loss = 2.65979 (* 1 = 2.65979 loss)
I1010 14:31:25.114583  5799 sgd_solver.cpp:106] Iteration 150, lr = 0.0001
I1010 14:31:29.271987  5799 solver.cpp:228] Iteration 200, loss = 2.61479
I1010 14:31:29.272009  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:31:29.272017  5799 solver.cpp:244]     Train net output #1: loss = 2.61479 (* 1 = 2.61479 loss)
I1010 14:31:29.272065  5799 sgd_solver.cpp:106] Iteration 200, lr = 0.0001
I1010 14:31:33.401708  5799 solver.cpp:337] Iteration 250, Testing net (#0)
I1010 14:31:36.901892  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1026
I1010 14:31:36.901926  5799 solver.cpp:404]     Test net output #1: loss = 2.36303 (* 1 = 2.36303 loss)
I1010 14:31:36.929730  5799 solver.cpp:228] Iteration 250, loss = 2.50368
I1010 14:31:36.929765  5799 solver.cpp:244]     Train net output #0: accuracy = 0.125
I1010 14:31:36.929772  5799 solver.cpp:244]     Train net output #1: loss = 2.50368 (* 1 = 2.50368 loss)
I1010 14:31:36.929777  5799 sgd_solver.cpp:106] Iteration 250, lr = 0.0001
I1010 14:31:41.086539  5799 solver.cpp:228] Iteration 300, loss = 2.53132
I1010 14:31:41.086657  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:31:41.086665  5799 solver.cpp:244]     Train net output #1: loss = 2.53132 (* 1 = 2.53132 loss)
I1010 14:31:41.086669  5799 sgd_solver.cpp:106] Iteration 300, lr = 0.0001
I1010 14:31:45.242089  5799 solver.cpp:228] Iteration 350, loss = 2.70912
I1010 14:31:45.242125  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:31:45.242132  5799 solver.cpp:244]     Train net output #1: loss = 2.70912 (* 1 = 2.70912 loss)
I1010 14:31:45.242136  5799 sgd_solver.cpp:106] Iteration 350, lr = 0.0001
I1010 14:31:49.400558  5799 solver.cpp:228] Iteration 400, loss = 2.66844
I1010 14:31:49.400594  5799 solver.cpp:244]     Train net output #0: accuracy = 0.125
I1010 14:31:49.400601  5799 solver.cpp:244]     Train net output #1: loss = 2.66844 (* 1 = 2.66844 loss)
I1010 14:31:49.400605  5799 sgd_solver.cpp:106] Iteration 400, lr = 0.0001
I1010 14:31:53.557564  5799 solver.cpp:228] Iteration 450, loss = 2.76251
I1010 14:31:53.557585  5799 solver.cpp:244]     Train net output #0: accuracy = 0.046875
I1010 14:31:53.557592  5799 solver.cpp:244]     Train net output #1: loss = 2.76251 (* 1 = 2.76251 loss)
I1010 14:31:53.557610  5799 sgd_solver.cpp:106] Iteration 450, lr = 0.0001
I1010 14:31:57.688514  5799 solver.cpp:337] Iteration 500, Testing net (#0)
I1010 14:32:01.191123  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1028
I1010 14:32:01.191159  5799 solver.cpp:404]     Test net output #1: loss = 2.35669 (* 1 = 2.35669 loss)
I1010 14:32:01.218822  5799 solver.cpp:228] Iteration 500, loss = 2.51058
I1010 14:32:01.218840  5799 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:32:01.218847  5799 solver.cpp:244]     Train net output #1: loss = 2.51058 (* 1 = 2.51058 loss)
I1010 14:32:01.218852  5799 sgd_solver.cpp:106] Iteration 500, lr = 0.0001
I1010 14:32:05.377988  5799 solver.cpp:228] Iteration 550, loss = 2.50213
I1010 14:32:05.378010  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:32:05.378017  5799 solver.cpp:244]     Train net output #1: loss = 2.50213 (* 1 = 2.50213 loss)
I1010 14:32:05.378036  5799 sgd_solver.cpp:106] Iteration 550, lr = 0.0001
I1010 14:32:09.537974  5799 solver.cpp:228] Iteration 600, loss = 2.70314
I1010 14:32:09.538009  5799 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:32:09.538017  5799 solver.cpp:244]     Train net output #1: loss = 2.70314 (* 1 = 2.70314 loss)
I1010 14:32:09.538020  5799 sgd_solver.cpp:106] Iteration 600, lr = 0.0001
I1010 14:32:13.740993  5799 solver.cpp:228] Iteration 650, loss = 2.44926
I1010 14:32:13.741039  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:32:13.741046  5799 solver.cpp:244]     Train net output #1: loss = 2.44926 (* 1 = 2.44926 loss)
I1010 14:32:13.741050  5799 sgd_solver.cpp:106] Iteration 650, lr = 0.0001
I1010 14:32:17.929344  5799 solver.cpp:228] Iteration 700, loss = 2.65489
I1010 14:32:17.929365  5799 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:32:17.929386  5799 solver.cpp:244]     Train net output #1: loss = 2.65489 (* 1 = 2.65489 loss)
I1010 14:32:17.929390  5799 sgd_solver.cpp:106] Iteration 700, lr = 0.0001
I1010 14:32:22.088184  5799 solver.cpp:337] Iteration 750, Testing net (#0)
I1010 14:32:25.599620  5799 solver.cpp:404]     Test net output #0: accuracy = 0.103
I1010 14:32:25.599642  5799 solver.cpp:404]     Test net output #1: loss = 2.35139 (* 1 = 2.35139 loss)
I1010 14:32:25.629448  5799 solver.cpp:228] Iteration 750, loss = 2.50505
I1010 14:32:25.629468  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:32:25.629474  5799 solver.cpp:244]     Train net output #1: loss = 2.50505 (* 1 = 2.50505 loss)
I1010 14:32:25.629478  5799 sgd_solver.cpp:106] Iteration 750, lr = 0.0001
I1010 14:32:29.795255  5799 solver.cpp:228] Iteration 800, loss = 2.37133
I1010 14:32:29.795275  5799 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:32:29.795296  5799 solver.cpp:244]     Train net output #1: loss = 2.37133 (* 1 = 2.37133 loss)
I1010 14:32:29.795300  5799 sgd_solver.cpp:106] Iteration 800, lr = 0.0001
I1010 14:32:33.954447  5799 solver.cpp:228] Iteration 850, loss = 2.60644
I1010 14:32:33.954468  5799 solver.cpp:244]     Train net output #0: accuracy = 0.0625
I1010 14:32:33.954475  5799 solver.cpp:244]     Train net output #1: loss = 2.60644 (* 1 = 2.60644 loss)
I1010 14:32:33.954478  5799 sgd_solver.cpp:106] Iteration 850, lr = 0.0001
I1010 14:32:38.156924  5799 solver.cpp:228] Iteration 900, loss = 2.45338
I1010 14:32:38.156945  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:32:38.156954  5799 solver.cpp:244]     Train net output #1: loss = 2.45338 (* 1 = 2.45338 loss)
I1010 14:32:38.156971  5799 sgd_solver.cpp:106] Iteration 900, lr = 0.0001
I1010 14:32:42.361554  5799 solver.cpp:228] Iteration 950, loss = 2.64905
I1010 14:32:42.361573  5799 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:32:42.361580  5799 solver.cpp:244]     Train net output #1: loss = 2.64905 (* 1 = 2.64905 loss)
I1010 14:32:42.361584  5799 sgd_solver.cpp:106] Iteration 950, lr = 0.0001
I1010 14:32:46.497998  5799 solver.cpp:337] Iteration 1000, Testing net (#0)
I1010 14:32:50.001363  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1035
I1010 14:32:50.001384  5799 solver.cpp:404]     Test net output #1: loss = 2.34616 (* 1 = 2.34616 loss)
I1010 14:32:50.027745  5799 solver.cpp:228] Iteration 1000, loss = 2.63159
I1010 14:32:50.027757  5799 solver.cpp:244]     Train net output #0: accuracy = 0.0625
I1010 14:32:50.027778  5799 solver.cpp:244]     Train net output #1: loss = 2.63159 (* 1 = 2.63159 loss)
I1010 14:32:50.027783  5799 sgd_solver.cpp:106] Iteration 1000, lr = 0.0001
I1010 14:32:54.193933  5799 solver.cpp:228] Iteration 1050, loss = 2.28102
I1010 14:32:54.193969  5799 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I1010 14:32:54.193976  5799 solver.cpp:244]     Train net output #1: loss = 2.28102 (* 1 = 2.28102 loss)
I1010 14:32:54.193994  5799 sgd_solver.cpp:106] Iteration 1050, lr = 0.0001
I1010 14:32:58.383072  5799 solver.cpp:228] Iteration 1100, loss = 2.60304
I1010 14:32:58.383093  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:32:58.383100  5799 solver.cpp:244]     Train net output #1: loss = 2.60304 (* 1 = 2.60304 loss)
I1010 14:32:58.383105  5799 sgd_solver.cpp:106] Iteration 1100, lr = 0.0001
I1010 14:33:02.553643  5799 solver.cpp:228] Iteration 1150, loss = 2.44748
I1010 14:33:02.553668  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:33:02.553675  5799 solver.cpp:244]     Train net output #1: loss = 2.44748 (* 1 = 2.44748 loss)
I1010 14:33:02.553680  5799 sgd_solver.cpp:106] Iteration 1150, lr = 0.0001
I1010 14:33:06.735441  5799 solver.cpp:228] Iteration 1200, loss = 2.45101
I1010 14:33:06.735462  5799 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:33:06.735471  5799 solver.cpp:244]     Train net output #1: loss = 2.45101 (* 1 = 2.45101 loss)
I1010 14:33:06.735473  5799 sgd_solver.cpp:106] Iteration 1200, lr = 0.0001
I1010 14:33:10.875417  5799 solver.cpp:337] Iteration 1250, Testing net (#0)
I1010 14:33:14.386018  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1037
I1010 14:33:14.386040  5799 solver.cpp:404]     Test net output #1: loss = 2.34132 (* 1 = 2.34132 loss)
I1010 14:33:14.415956  5799 solver.cpp:228] Iteration 1250, loss = 2.53948
I1010 14:33:14.416007  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:33:14.416029  5799 solver.cpp:244]     Train net output #1: loss = 2.53948 (* 1 = 2.53948 loss)
I1010 14:33:14.416034  5799 sgd_solver.cpp:106] Iteration 1250, lr = 0.0001
I1010 14:33:18.584132  5799 solver.cpp:228] Iteration 1300, loss = 2.38995
I1010 14:33:18.584236  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:33:18.584245  5799 solver.cpp:244]     Train net output #1: loss = 2.38995 (* 1 = 2.38995 loss)
I1010 14:33:18.584249  5799 sgd_solver.cpp:106] Iteration 1300, lr = 0.0001
I1010 14:33:22.750229  5799 solver.cpp:228] Iteration 1350, loss = 2.52257
I1010 14:33:22.750249  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:33:22.750272  5799 solver.cpp:244]     Train net output #1: loss = 2.52257 (* 1 = 2.52257 loss)
I1010 14:33:22.750274  5799 sgd_solver.cpp:106] Iteration 1350, lr = 0.0001
I1010 14:33:26.915901  5799 solver.cpp:228] Iteration 1400, loss = 2.59697
I1010 14:33:26.915935  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:33:26.915943  5799 solver.cpp:244]     Train net output #1: loss = 2.59697 (* 1 = 2.59697 loss)
I1010 14:33:26.915946  5799 sgd_solver.cpp:106] Iteration 1400, lr = 0.0001
I1010 14:33:31.083936  5799 solver.cpp:228] Iteration 1450, loss = 2.59548
I1010 14:33:31.083957  5799 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:33:31.083964  5799 solver.cpp:244]     Train net output #1: loss = 2.59548 (* 1 = 2.59548 loss)
I1010 14:33:31.083968  5799 sgd_solver.cpp:106] Iteration 1450, lr = 0.0001
I1010 14:33:35.226339  5799 solver.cpp:337] Iteration 1500, Testing net (#0)
I1010 14:33:38.740809  5799 solver.cpp:404]     Test net output #0: accuracy = 0.104
I1010 14:33:38.740857  5799 solver.cpp:404]     Test net output #1: loss = 2.3372 (* 1 = 2.3372 loss)
I1010 14:33:38.770781  5799 solver.cpp:228] Iteration 1500, loss = 2.62503
I1010 14:33:38.770799  5799 solver.cpp:244]     Train net output #0: accuracy = 0.046875
I1010 14:33:38.770807  5799 solver.cpp:244]     Train net output #1: loss = 2.62503 (* 1 = 2.62503 loss)
I1010 14:33:38.770810  5799 sgd_solver.cpp:106] Iteration 1500, lr = 0.0001
I1010 14:33:42.938714  5799 solver.cpp:228] Iteration 1550, loss = 2.57075
I1010 14:33:42.938750  5799 solver.cpp:244]     Train net output #0: accuracy = 0.0625
I1010 14:33:42.938756  5799 solver.cpp:244]     Train net output #1: loss = 2.57075 (* 1 = 2.57075 loss)
I1010 14:33:42.938760  5799 sgd_solver.cpp:106] Iteration 1550, lr = 0.0001
I1010 14:33:47.107708  5799 solver.cpp:228] Iteration 1600, loss = 2.45482
I1010 14:33:47.107728  5799 solver.cpp:244]     Train net output #0: accuracy = 0.21875
I1010 14:33:47.107735  5799 solver.cpp:244]     Train net output #1: loss = 2.45482 (* 1 = 2.45482 loss)
I1010 14:33:47.107739  5799 sgd_solver.cpp:106] Iteration 1600, lr = 0.0001
I1010 14:33:51.277452  5799 solver.cpp:228] Iteration 1650, loss = 2.45976
I1010 14:33:51.277554  5799 solver.cpp:244]     Train net output #0: accuracy = 0.125
I1010 14:33:51.277575  5799 solver.cpp:244]     Train net output #1: loss = 2.45976 (* 1 = 2.45976 loss)
I1010 14:33:51.277591  5799 sgd_solver.cpp:106] Iteration 1650, lr = 0.0001
I1010 14:33:55.447561  5799 solver.cpp:228] Iteration 1700, loss = 2.44863
I1010 14:33:55.447580  5799 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:33:55.447588  5799 solver.cpp:244]     Train net output #1: loss = 2.44863 (* 1 = 2.44863 loss)
I1010 14:33:55.447605  5799 sgd_solver.cpp:106] Iteration 1700, lr = 0.0001
I1010 14:33:59.591346  5799 solver.cpp:337] Iteration 1750, Testing net (#0)
I1010 14:34:03.106878  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1044
I1010 14:34:03.106911  5799 solver.cpp:404]     Test net output #1: loss = 2.33337 (* 1 = 2.33337 loss)
I1010 14:34:03.136778  5799 solver.cpp:228] Iteration 1750, loss = 2.33759
I1010 14:34:03.136797  5799 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:34:03.136804  5799 solver.cpp:244]     Train net output #1: loss = 2.33759 (* 1 = 2.33759 loss)
I1010 14:34:03.136808  5799 sgd_solver.cpp:106] Iteration 1750, lr = 0.0001
I1010 14:34:07.305901  5799 solver.cpp:228] Iteration 1800, loss = 2.48854
I1010 14:34:07.305920  5799 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:34:07.305927  5799 solver.cpp:244]     Train net output #1: loss = 2.48854 (* 1 = 2.48854 loss)
I1010 14:34:07.305930  5799 sgd_solver.cpp:106] Iteration 1800, lr = 0.0001
I1010 14:34:11.479534  5799 solver.cpp:228] Iteration 1850, loss = 2.53695
I1010 14:34:11.479555  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:34:11.479562  5799 solver.cpp:244]     Train net output #1: loss = 2.53695 (* 1 = 2.53695 loss)
I1010 14:34:11.479565  5799 sgd_solver.cpp:106] Iteration 1850, lr = 0.0001
I1010 14:34:15.646502  5799 solver.cpp:228] Iteration 1900, loss = 2.42588
I1010 14:34:15.646522  5799 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:34:15.646528  5799 solver.cpp:244]     Train net output #1: loss = 2.42588 (* 1 = 2.42588 loss)
I1010 14:34:15.646531  5799 sgd_solver.cpp:106] Iteration 1900, lr = 0.0001
I1010 14:34:19.814867  5799 solver.cpp:228] Iteration 1950, loss = 2.44675
I1010 14:34:19.814888  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:34:19.814894  5799 solver.cpp:244]     Train net output #1: loss = 2.44675 (* 1 = 2.44675 loss)
I1010 14:34:19.814913  5799 sgd_solver.cpp:106] Iteration 1950, lr = 0.0001
I1010 14:34:23.957236  5799 solver.cpp:337] Iteration 2000, Testing net (#0)
I1010 14:34:27.469296  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1052
I1010 14:34:27.469318  5799 solver.cpp:404]     Test net output #1: loss = 2.32986 (* 1 = 2.32986 loss)
I1010 14:34:27.499169  5799 solver.cpp:228] Iteration 2000, loss = 2.35664
I1010 14:34:27.499191  5799 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I1010 14:34:27.499197  5799 solver.cpp:244]     Train net output #1: loss = 2.35664 (* 1 = 2.35664 loss)
I1010 14:34:27.499202  5799 sgd_solver.cpp:106] Iteration 2000, lr = 0.0001
I1010 14:34:31.667686  5799 solver.cpp:228] Iteration 2050, loss = 2.52892
I1010 14:34:31.667707  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:34:31.667729  5799 solver.cpp:244]     Train net output #1: loss = 2.52892 (* 1 = 2.52892 loss)
I1010 14:34:31.667733  5799 sgd_solver.cpp:106] Iteration 2050, lr = 0.0001
I1010 14:34:35.834944  5799 solver.cpp:228] Iteration 2100, loss = 2.52068
I1010 14:34:35.834978  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:34:35.834985  5799 solver.cpp:244]     Train net output #1: loss = 2.52068 (* 1 = 2.52068 loss)
I1010 14:34:35.834990  5799 sgd_solver.cpp:106] Iteration 2100, lr = 0.0001
I1010 14:34:40.001749  5799 solver.cpp:228] Iteration 2150, loss = 2.49907
I1010 14:34:40.001770  5799 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:34:40.001776  5799 solver.cpp:244]     Train net output #1: loss = 2.49907 (* 1 = 2.49907 loss)
I1010 14:34:40.001780  5799 sgd_solver.cpp:106] Iteration 2150, lr = 0.0001
I1010 14:34:44.163492  5799 solver.cpp:228] Iteration 2200, loss = 2.3681
I1010 14:34:44.163512  5799 solver.cpp:244]     Train net output #0: accuracy = 0.125
I1010 14:34:44.163533  5799 solver.cpp:244]     Train net output #1: loss = 2.3681 (* 1 = 2.3681 loss)
I1010 14:34:44.163537  5799 sgd_solver.cpp:106] Iteration 2200, lr = 0.0001
I1010 14:34:48.303059  5799 solver.cpp:337] Iteration 2250, Testing net (#0)
I1010 14:34:51.816081  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1052
I1010 14:34:51.816117  5799 solver.cpp:404]     Test net output #1: loss = 2.32674 (* 1 = 2.32674 loss)
I1010 14:34:51.846145  5799 solver.cpp:228] Iteration 2250, loss = 2.37418
I1010 14:34:51.846165  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:34:51.846173  5799 solver.cpp:244]     Train net output #1: loss = 2.37418 (* 1 = 2.37418 loss)
I1010 14:34:51.846176  5799 sgd_solver.cpp:106] Iteration 2250, lr = 0.0001
I1010 14:34:56.012555  5799 solver.cpp:228] Iteration 2300, loss = 2.58318
I1010 14:34:56.012639  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:34:56.012650  5799 solver.cpp:244]     Train net output #1: loss = 2.58318 (* 1 = 2.58318 loss)
I1010 14:34:56.012666  5799 sgd_solver.cpp:106] Iteration 2300, lr = 0.0001
I1010 14:35:00.179172  5799 solver.cpp:228] Iteration 2350, loss = 2.37244
I1010 14:35:00.179191  5799 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:35:00.179198  5799 solver.cpp:244]     Train net output #1: loss = 2.37244 (* 1 = 2.37244 loss)
I1010 14:35:00.179203  5799 sgd_solver.cpp:106] Iteration 2350, lr = 0.0001
I1010 14:35:04.349594  5799 solver.cpp:228] Iteration 2400, loss = 2.41674
I1010 14:35:04.349616  5799 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:35:04.349622  5799 solver.cpp:244]     Train net output #1: loss = 2.41674 (* 1 = 2.41674 loss)
I1010 14:35:04.349640  5799 sgd_solver.cpp:106] Iteration 2400, lr = 0.0001
I1010 14:35:08.514156  5799 solver.cpp:228] Iteration 2450, loss = 2.48067
I1010 14:35:08.514176  5799 solver.cpp:244]     Train net output #0: accuracy = 0.046875
I1010 14:35:08.514183  5799 solver.cpp:244]     Train net output #1: loss = 2.48067 (* 1 = 2.48067 loss)
I1010 14:35:08.514186  5799 sgd_solver.cpp:106] Iteration 2450, lr = 0.0001
I1010 14:35:12.653120  5799 solver.cpp:454] Snapshotting to binary proto file cifar-10_relu_AdaDelta_iter_2500.caffemodel
I1010 14:35:12.675122  5799 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar-10_relu_AdaDelta_iter_2500.solverstate
I1010 14:35:12.715327  5799 solver.cpp:317] Iteration 2500, loss = 2.35319
I1010 14:35:12.715348  5799 solver.cpp:337] Iteration 2500, Testing net (#0)
I1010 14:35:16.225628  5799 solver.cpp:404]     Test net output #0: accuracy = 0.1053
I1010 14:35:16.225651  5799 solver.cpp:404]     Test net output #1: loss = 2.32396 (* 1 = 2.32396 loss)
I1010 14:35:16.225656  5799 solver.cpp:322] Optimization Done.
I1010 14:35:16.225657  5799 caffe.cpp:254] Optimization Done.
