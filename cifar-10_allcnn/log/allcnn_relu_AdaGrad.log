I1010 14:35:16.334733  5852 caffe.cpp:217] Using GPUs 0
I1010 14:35:16.337421  5852 caffe.cpp:222] GPU 0: GeForce GTX 750 Ti
I1010 14:35:16.445816  5852 solver.cpp:48] Initializing solver from parameters: 
train_net: "/home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_train.prototxt"
test_net: "/home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_test.prototxt"
test_iter: 100
test_interval: 250
base_lr: 0.0001
display: 50
max_iter: 2500
lr_policy: "step"
gamma: 0.1
weight_decay: 0.0005
stepsize: 5000
snapshot: 5000
snapshot_prefix: "cifar-10_relu_AdaGrad"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
type: "AdaGrad"
I1010 14:35:16.445966  5852 solver.cpp:81] Creating training net from train_net file: /home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_train.prototxt
I1010 14:35:16.446283  5852 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "../../../data/cifar-10/cifar10_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn1"
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "bn1"
  top: "bn1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "bn1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv5"
  top: "bn2"
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "bn2"
  top: "bn2"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "bn2"
  top: "drop4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "drop4"
  top: "conv6"
  convolution_param {
    num_output: 192
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "conv6"
  top: "conv6"
}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "conv6"
  top: "conv7"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "conv7"
  top: "conv8"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "conv8"
  top: "conv9"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}
layer {
  name: "pool"
  type: "Pooling"
  bottom: "conv9"
  top: "pool"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "flatten"
  type: "Flatten"
  bottom: "pool"
  top: "flatten"
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "flatten"
  top: "score"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
I1010 14:35:16.446442  5852 layer_factory.hpp:77] Creating layer data
I1010 14:35:16.446877  5852 net.cpp:100] Creating Layer data
I1010 14:35:16.446899  5852 net.cpp:408] data -> data
I1010 14:35:16.446929  5852 net.cpp:408] data -> label
I1010 14:35:16.446954  5852 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I1010 14:35:16.447600  5857 db_lmdb.cpp:35] Opened lmdb ../../../data/cifar-10/cifar10_train_lmdb
I1010 14:35:16.454064  5852 data_layer.cpp:41] output data size: 64,3,32,32
I1010 14:35:16.455708  5852 net.cpp:150] Setting up data
I1010 14:35:16.455726  5852 net.cpp:157] Top shape: 64 3 32 32 (196608)
I1010 14:35:16.455730  5852 net.cpp:157] Top shape: 64 (64)
I1010 14:35:16.455732  5852 net.cpp:165] Memory required for data: 786688
I1010 14:35:16.455754  5852 layer_factory.hpp:77] Creating layer label_data_1_split
I1010 14:35:16.455776  5852 net.cpp:100] Creating Layer label_data_1_split
I1010 14:35:16.455780  5852 net.cpp:434] label_data_1_split <- label
I1010 14:35:16.455803  5852 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1010 14:35:16.455811  5852 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1010 14:35:16.455862  5852 net.cpp:150] Setting up label_data_1_split
I1010 14:35:16.455868  5852 net.cpp:157] Top shape: 64 (64)
I1010 14:35:16.455870  5852 net.cpp:157] Top shape: 64 (64)
I1010 14:35:16.455871  5852 net.cpp:165] Memory required for data: 787200
I1010 14:35:16.455873  5852 layer_factory.hpp:77] Creating layer conv1
I1010 14:35:16.455885  5852 net.cpp:100] Creating Layer conv1
I1010 14:35:16.455888  5852 net.cpp:434] conv1 <- data
I1010 14:35:16.455891  5852 net.cpp:408] conv1 -> conv1
I1010 14:35:16.591433  5852 net.cpp:150] Setting up conv1
I1010 14:35:16.591456  5852 net.cpp:157] Top shape: 64 96 30 30 (5529600)
I1010 14:35:16.591459  5852 net.cpp:165] Memory required for data: 22905600
I1010 14:35:16.591476  5852 layer_factory.hpp:77] Creating layer relu1
I1010 14:35:16.591485  5852 net.cpp:100] Creating Layer relu1
I1010 14:35:16.591487  5852 net.cpp:434] relu1 <- conv1
I1010 14:35:16.591490  5852 net.cpp:395] relu1 -> conv1 (in-place)
I1010 14:35:16.591666  5852 net.cpp:150] Setting up relu1
I1010 14:35:16.591672  5852 net.cpp:157] Top shape: 64 96 30 30 (5529600)
I1010 14:35:16.591675  5852 net.cpp:165] Memory required for data: 45024000
I1010 14:35:16.591676  5852 layer_factory.hpp:77] Creating layer conv2
I1010 14:35:16.591686  5852 net.cpp:100] Creating Layer conv2
I1010 14:35:16.591687  5852 net.cpp:434] conv2 <- conv1
I1010 14:35:16.591691  5852 net.cpp:408] conv2 -> conv2
I1010 14:35:16.593096  5852 net.cpp:150] Setting up conv2
I1010 14:35:16.593106  5852 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1010 14:35:16.593107  5852 net.cpp:165] Memory required for data: 64291584
I1010 14:35:16.593114  5852 layer_factory.hpp:77] Creating layer bn1
I1010 14:35:16.593118  5852 net.cpp:100] Creating Layer bn1
I1010 14:35:16.593121  5852 net.cpp:434] bn1 <- conv2
I1010 14:35:16.593124  5852 net.cpp:408] bn1 -> bn1
I1010 14:35:16.593319  5852 net.cpp:150] Setting up bn1
I1010 14:35:16.593325  5852 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1010 14:35:16.593327  5852 net.cpp:165] Memory required for data: 83559168
I1010 14:35:16.593333  5852 layer_factory.hpp:77] Creating layer relu2
I1010 14:35:16.593338  5852 net.cpp:100] Creating Layer relu2
I1010 14:35:16.593339  5852 net.cpp:434] relu2 <- bn1
I1010 14:35:16.593341  5852 net.cpp:395] relu2 -> bn1 (in-place)
I1010 14:35:16.593518  5852 net.cpp:150] Setting up relu2
I1010 14:35:16.593523  5852 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1010 14:35:16.593525  5852 net.cpp:165] Memory required for data: 102826752
I1010 14:35:16.593538  5852 layer_factory.hpp:77] Creating layer drop1
I1010 14:35:16.593542  5852 net.cpp:100] Creating Layer drop1
I1010 14:35:16.593544  5852 net.cpp:434] drop1 <- bn1
I1010 14:35:16.593547  5852 net.cpp:408] drop1 -> drop1
I1010 14:35:16.593590  5852 net.cpp:150] Setting up drop1
I1010 14:35:16.593595  5852 net.cpp:157] Top shape: 64 96 28 28 (4816896)
I1010 14:35:16.593596  5852 net.cpp:165] Memory required for data: 122094336
I1010 14:35:16.593612  5852 layer_factory.hpp:77] Creating layer conv3
I1010 14:35:16.593619  5852 net.cpp:100] Creating Layer conv3
I1010 14:35:16.593634  5852 net.cpp:434] conv3 <- drop1
I1010 14:35:16.593637  5852 net.cpp:408] conv3 -> conv3
I1010 14:35:16.594817  5852 net.cpp:150] Setting up conv3
I1010 14:35:16.594827  5852 net.cpp:157] Top shape: 64 96 13 13 (1038336)
I1010 14:35:16.594830  5852 net.cpp:165] Memory required for data: 126247680
I1010 14:35:16.594835  5852 layer_factory.hpp:77] Creating layer relu3
I1010 14:35:16.594840  5852 net.cpp:100] Creating Layer relu3
I1010 14:35:16.594841  5852 net.cpp:434] relu3 <- conv3
I1010 14:35:16.594844  5852 net.cpp:395] relu3 -> conv3 (in-place)
I1010 14:35:16.595104  5852 net.cpp:150] Setting up relu3
I1010 14:35:16.595113  5852 net.cpp:157] Top shape: 64 96 13 13 (1038336)
I1010 14:35:16.595115  5852 net.cpp:165] Memory required for data: 130401024
I1010 14:35:16.595118  5852 layer_factory.hpp:77] Creating layer conv4
I1010 14:35:16.595122  5852 net.cpp:100] Creating Layer conv4
I1010 14:35:16.595124  5852 net.cpp:434] conv4 <- conv3
I1010 14:35:16.595129  5852 net.cpp:408] conv4 -> conv4
I1010 14:35:16.596943  5852 net.cpp:150] Setting up conv4
I1010 14:35:16.596953  5852 net.cpp:157] Top shape: 64 192 11 11 (1486848)
I1010 14:35:16.596956  5852 net.cpp:165] Memory required for data: 136348416
I1010 14:35:16.596976  5852 layer_factory.hpp:77] Creating layer relu4
I1010 14:35:16.596978  5852 net.cpp:100] Creating Layer relu4
I1010 14:35:16.596982  5852 net.cpp:434] relu4 <- conv4
I1010 14:35:16.596997  5852 net.cpp:395] relu4 -> conv4 (in-place)
I1010 14:35:16.597163  5852 net.cpp:150] Setting up relu4
I1010 14:35:16.597169  5852 net.cpp:157] Top shape: 64 192 11 11 (1486848)
I1010 14:35:16.597172  5852 net.cpp:165] Memory required for data: 142295808
I1010 14:35:16.597174  5852 layer_factory.hpp:77] Creating layer conv5
I1010 14:35:16.597179  5852 net.cpp:100] Creating Layer conv5
I1010 14:35:16.597182  5852 net.cpp:434] conv5 <- conv4
I1010 14:35:16.597184  5852 net.cpp:408] conv5 -> conv5
I1010 14:35:16.599706  5852 net.cpp:150] Setting up conv5
I1010 14:35:16.599717  5852 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1010 14:35:16.599720  5852 net.cpp:165] Memory required for data: 146277120
I1010 14:35:16.599723  5852 layer_factory.hpp:77] Creating layer bn2
I1010 14:35:16.599728  5852 net.cpp:100] Creating Layer bn2
I1010 14:35:16.599731  5852 net.cpp:434] bn2 <- conv5
I1010 14:35:16.599735  5852 net.cpp:408] bn2 -> bn2
I1010 14:35:16.599947  5852 net.cpp:150] Setting up bn2
I1010 14:35:16.599953  5852 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1010 14:35:16.599956  5852 net.cpp:165] Memory required for data: 150258432
I1010 14:35:16.599959  5852 layer_factory.hpp:77] Creating layer relu5
I1010 14:35:16.599963  5852 net.cpp:100] Creating Layer relu5
I1010 14:35:16.599966  5852 net.cpp:434] relu5 <- bn2
I1010 14:35:16.599968  5852 net.cpp:395] relu5 -> bn2 (in-place)
I1010 14:35:16.600153  5852 net.cpp:150] Setting up relu5
I1010 14:35:16.600159  5852 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1010 14:35:16.600162  5852 net.cpp:165] Memory required for data: 154239744
I1010 14:35:16.600163  5852 layer_factory.hpp:77] Creating layer drop4
I1010 14:35:16.600167  5852 net.cpp:100] Creating Layer drop4
I1010 14:35:16.600169  5852 net.cpp:434] drop4 <- bn2
I1010 14:35:16.600173  5852 net.cpp:408] drop4 -> drop4
I1010 14:35:16.600217  5852 net.cpp:150] Setting up drop4
I1010 14:35:16.600222  5852 net.cpp:157] Top shape: 64 192 9 9 (995328)
I1010 14:35:16.600236  5852 net.cpp:165] Memory required for data: 158221056
I1010 14:35:16.600263  5852 layer_factory.hpp:77] Creating layer conv6
I1010 14:35:16.600286  5852 net.cpp:100] Creating Layer conv6
I1010 14:35:16.600287  5852 net.cpp:434] conv6 <- drop4
I1010 14:35:16.600291  5852 net.cpp:408] conv6 -> conv6
I1010 14:35:16.602859  5852 net.cpp:150] Setting up conv6
I1010 14:35:16.602885  5852 net.cpp:157] Top shape: 64 192 4 4 (196608)
I1010 14:35:16.602888  5852 net.cpp:165] Memory required for data: 159007488
I1010 14:35:16.602897  5852 layer_factory.hpp:77] Creating layer relu6
I1010 14:35:16.602902  5852 net.cpp:100] Creating Layer relu6
I1010 14:35:16.602906  5852 net.cpp:434] relu6 <- conv6
I1010 14:35:16.602924  5852 net.cpp:395] relu6 -> conv6 (in-place)
I1010 14:35:16.603179  5852 net.cpp:150] Setting up relu6
I1010 14:35:16.603188  5852 net.cpp:157] Top shape: 64 192 4 4 (196608)
I1010 14:35:16.603190  5852 net.cpp:165] Memory required for data: 159793920
I1010 14:35:16.603193  5852 layer_factory.hpp:77] Creating layer conv7
I1010 14:35:16.603212  5852 net.cpp:100] Creating Layer conv7
I1010 14:35:16.603214  5852 net.cpp:434] conv7 <- conv6
I1010 14:35:16.603220  5852 net.cpp:408] conv7 -> conv7
I1010 14:35:16.606362  5852 net.cpp:150] Setting up conv7
I1010 14:35:16.606380  5852 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1010 14:35:16.606384  5852 net.cpp:165] Memory required for data: 159990528
I1010 14:35:16.606389  5852 layer_factory.hpp:77] Creating layer relu7
I1010 14:35:16.606395  5852 net.cpp:100] Creating Layer relu7
I1010 14:35:16.606397  5852 net.cpp:434] relu7 <- conv7
I1010 14:35:16.606400  5852 net.cpp:395] relu7 -> conv7 (in-place)
I1010 14:35:16.606657  5852 net.cpp:150] Setting up relu7
I1010 14:35:16.606665  5852 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1010 14:35:16.606668  5852 net.cpp:165] Memory required for data: 160187136
I1010 14:35:16.606670  5852 layer_factory.hpp:77] Creating layer conv8
I1010 14:35:16.606678  5852 net.cpp:100] Creating Layer conv8
I1010 14:35:16.606679  5852 net.cpp:434] conv8 <- conv7
I1010 14:35:16.606683  5852 net.cpp:408] conv8 -> conv8
I1010 14:35:16.607941  5852 net.cpp:150] Setting up conv8
I1010 14:35:16.607951  5852 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1010 14:35:16.607954  5852 net.cpp:165] Memory required for data: 160383744
I1010 14:35:16.607957  5852 layer_factory.hpp:77] Creating layer relu8
I1010 14:35:16.607961  5852 net.cpp:100] Creating Layer relu8
I1010 14:35:16.607964  5852 net.cpp:434] relu8 <- conv8
I1010 14:35:16.607969  5852 net.cpp:395] relu8 -> conv8 (in-place)
I1010 14:35:16.608137  5852 net.cpp:150] Setting up relu8
I1010 14:35:16.608144  5852 net.cpp:157] Top shape: 64 192 2 2 (49152)
I1010 14:35:16.608146  5852 net.cpp:165] Memory required for data: 160580352
I1010 14:35:16.608147  5852 layer_factory.hpp:77] Creating layer conv9
I1010 14:35:16.608153  5852 net.cpp:100] Creating Layer conv9
I1010 14:35:16.608155  5852 net.cpp:434] conv9 <- conv8
I1010 14:35:16.608160  5852 net.cpp:408] conv9 -> conv9
I1010 14:35:16.608976  5852 net.cpp:150] Setting up conv9
I1010 14:35:16.608988  5852 net.cpp:157] Top shape: 64 10 2 2 (2560)
I1010 14:35:16.608989  5852 net.cpp:165] Memory required for data: 160590592
I1010 14:35:16.608994  5852 layer_factory.hpp:77] Creating layer relu9
I1010 14:35:16.608997  5852 net.cpp:100] Creating Layer relu9
I1010 14:35:16.608999  5852 net.cpp:434] relu9 <- conv9
I1010 14:35:16.609002  5852 net.cpp:395] relu9 -> conv9 (in-place)
I1010 14:35:16.609253  5852 net.cpp:150] Setting up relu9
I1010 14:35:16.609262  5852 net.cpp:157] Top shape: 64 10 2 2 (2560)
I1010 14:35:16.609264  5852 net.cpp:165] Memory required for data: 160600832
I1010 14:35:16.609266  5852 layer_factory.hpp:77] Creating layer pool
I1010 14:35:16.609272  5852 net.cpp:100] Creating Layer pool
I1010 14:35:16.609273  5852 net.cpp:434] pool <- conv9
I1010 14:35:16.609277  5852 net.cpp:408] pool -> pool
I1010 14:35:16.609485  5852 net.cpp:150] Setting up pool
I1010 14:35:16.609491  5852 net.cpp:157] Top shape: 64 10 1 1 (640)
I1010 14:35:16.609493  5852 net.cpp:165] Memory required for data: 160603392
I1010 14:35:16.609504  5852 layer_factory.hpp:77] Creating layer flatten
I1010 14:35:16.609508  5852 net.cpp:100] Creating Layer flatten
I1010 14:35:16.609510  5852 net.cpp:434] flatten <- pool
I1010 14:35:16.609514  5852 net.cpp:408] flatten -> flatten
I1010 14:35:16.609549  5852 net.cpp:150] Setting up flatten
I1010 14:35:16.609555  5852 net.cpp:157] Top shape: 64 10 (640)
I1010 14:35:16.609555  5852 net.cpp:165] Memory required for data: 160605952
I1010 14:35:16.609557  5852 layer_factory.hpp:77] Creating layer score
I1010 14:35:16.609562  5852 net.cpp:100] Creating Layer score
I1010 14:35:16.609565  5852 net.cpp:434] score <- flatten
I1010 14:35:16.609585  5852 net.cpp:408] score -> score
I1010 14:35:16.609732  5852 net.cpp:150] Setting up score
I1010 14:35:16.609737  5852 net.cpp:157] Top shape: 64 10 (640)
I1010 14:35:16.609740  5852 net.cpp:165] Memory required for data: 160608512
I1010 14:35:16.609743  5852 layer_factory.hpp:77] Creating layer score_score_0_split
I1010 14:35:16.609746  5852 net.cpp:100] Creating Layer score_score_0_split
I1010 14:35:16.609748  5852 net.cpp:434] score_score_0_split <- score
I1010 14:35:16.609752  5852 net.cpp:408] score_score_0_split -> score_score_0_split_0
I1010 14:35:16.609756  5852 net.cpp:408] score_score_0_split -> score_score_0_split_1
I1010 14:35:16.609797  5852 net.cpp:150] Setting up score_score_0_split
I1010 14:35:16.609815  5852 net.cpp:157] Top shape: 64 10 (640)
I1010 14:35:16.609817  5852 net.cpp:157] Top shape: 64 10 (640)
I1010 14:35:16.609819  5852 net.cpp:165] Memory required for data: 160613632
I1010 14:35:16.609822  5852 layer_factory.hpp:77] Creating layer accuracy
I1010 14:35:16.609841  5852 net.cpp:100] Creating Layer accuracy
I1010 14:35:16.609843  5852 net.cpp:434] accuracy <- score_score_0_split_0
I1010 14:35:16.609846  5852 net.cpp:434] accuracy <- label_data_1_split_0
I1010 14:35:16.609849  5852 net.cpp:408] accuracy -> accuracy
I1010 14:35:16.609869  5852 net.cpp:150] Setting up accuracy
I1010 14:35:16.609871  5852 net.cpp:157] Top shape: (1)
I1010 14:35:16.609872  5852 net.cpp:165] Memory required for data: 160613636
I1010 14:35:16.609874  5852 layer_factory.hpp:77] Creating layer loss
I1010 14:35:16.609877  5852 net.cpp:100] Creating Layer loss
I1010 14:35:16.609899  5852 net.cpp:434] loss <- score_score_0_split_1
I1010 14:35:16.609901  5852 net.cpp:434] loss <- label_data_1_split_1
I1010 14:35:16.609905  5852 net.cpp:408] loss -> loss
I1010 14:35:16.609922  5852 layer_factory.hpp:77] Creating layer loss
I1010 14:35:16.610266  5852 net.cpp:150] Setting up loss
I1010 14:35:16.610275  5852 net.cpp:157] Top shape: (1)
I1010 14:35:16.610276  5852 net.cpp:160]     with loss weight 1
I1010 14:35:16.610288  5852 net.cpp:165] Memory required for data: 160613640
I1010 14:35:16.610291  5852 net.cpp:226] loss needs backward computation.
I1010 14:35:16.610296  5852 net.cpp:228] accuracy does not need backward computation.
I1010 14:35:16.610299  5852 net.cpp:226] score_score_0_split needs backward computation.
I1010 14:35:16.610301  5852 net.cpp:226] score needs backward computation.
I1010 14:35:16.610303  5852 net.cpp:226] flatten needs backward computation.
I1010 14:35:16.610304  5852 net.cpp:226] pool needs backward computation.
I1010 14:35:16.610306  5852 net.cpp:226] relu9 needs backward computation.
I1010 14:35:16.610323  5852 net.cpp:226] conv9 needs backward computation.
I1010 14:35:16.610324  5852 net.cpp:226] relu8 needs backward computation.
I1010 14:35:16.610327  5852 net.cpp:226] conv8 needs backward computation.
I1010 14:35:16.610328  5852 net.cpp:226] relu7 needs backward computation.
I1010 14:35:16.610330  5852 net.cpp:226] conv7 needs backward computation.
I1010 14:35:16.610332  5852 net.cpp:226] relu6 needs backward computation.
I1010 14:35:16.610334  5852 net.cpp:226] conv6 needs backward computation.
I1010 14:35:16.610337  5852 net.cpp:226] drop4 needs backward computation.
I1010 14:35:16.610352  5852 net.cpp:226] relu5 needs backward computation.
I1010 14:35:16.610354  5852 net.cpp:226] bn2 needs backward computation.
I1010 14:35:16.610364  5852 net.cpp:226] conv5 needs backward computation.
I1010 14:35:16.610366  5852 net.cpp:226] relu4 needs backward computation.
I1010 14:35:16.610381  5852 net.cpp:226] conv4 needs backward computation.
I1010 14:35:16.610383  5852 net.cpp:226] relu3 needs backward computation.
I1010 14:35:16.610385  5852 net.cpp:226] conv3 needs backward computation.
I1010 14:35:16.610388  5852 net.cpp:226] drop1 needs backward computation.
I1010 14:35:16.610404  5852 net.cpp:226] relu2 needs backward computation.
I1010 14:35:16.610406  5852 net.cpp:226] bn1 needs backward computation.
I1010 14:35:16.610409  5852 net.cpp:226] conv2 needs backward computation.
I1010 14:35:16.610410  5852 net.cpp:226] relu1 needs backward computation.
I1010 14:35:16.610425  5852 net.cpp:226] conv1 needs backward computation.
I1010 14:35:16.610429  5852 net.cpp:228] label_data_1_split does not need backward computation.
I1010 14:35:16.610430  5852 net.cpp:228] data does not need backward computation.
I1010 14:35:16.610450  5852 net.cpp:270] This network produces output accuracy
I1010 14:35:16.610451  5852 net.cpp:270] This network produces output loss
I1010 14:35:16.610467  5852 net.cpp:283] Network initialization done.
I1010 14:35:16.610718  5852 solver.cpp:181] Creating test net (#0) specified by test_net file: /home/adam/Honours/Honours-Project/cifar-10_allcnn/relu/allcnn_relu_test.prototxt
I1010 14:35:16.610874  5852 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "../../../data/cifar-10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  convolution_param {
    num_output: 96
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn1"
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "bn1"
  top: "bn1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "bn1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv5"
  top: "bn2"
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "bn2"
  top: "bn2"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "bn2"
  top: "drop4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "drop4"
  top: "conv6"
  convolution_param {
    num_output: 192
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "conv6"
  top: "conv6"
}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "conv6"
  top: "conv7"
  convolution_param {
    num_output: 192
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "conv7"
  top: "conv8"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "conv8"
  top: "conv9"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}
layer {
  name: "pool"
  type: "Pooling"
  bottom: "conv9"
  top: "pool"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "flatten"
  type: "Flatten"
  bottom: "pool"
  top: "flatten"
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "flatten"
  top: "score"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
I1010 14:35:16.610977  5852 layer_factory.hpp:77] Creating layer data
I1010 14:35:16.611114  5852 net.cpp:100] Creating Layer data
I1010 14:35:16.611121  5852 net.cpp:408] data -> data
I1010 14:35:16.611141  5852 net.cpp:408] data -> label
I1010 14:35:16.611148  5852 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I1010 14:35:16.611891  5859 db_lmdb.cpp:35] Opened lmdb ../../../data/cifar-10/cifar10_test_lmdb
I1010 14:35:16.612025  5852 data_layer.cpp:41] output data size: 100,3,32,32
I1010 14:35:16.613945  5852 net.cpp:150] Setting up data
I1010 14:35:16.613962  5852 net.cpp:157] Top shape: 100 3 32 32 (307200)
I1010 14:35:16.613965  5852 net.cpp:157] Top shape: 100 (100)
I1010 14:35:16.613967  5852 net.cpp:165] Memory required for data: 1229200
I1010 14:35:16.613984  5852 layer_factory.hpp:77] Creating layer label_data_1_split
I1010 14:35:16.613992  5852 net.cpp:100] Creating Layer label_data_1_split
I1010 14:35:16.613996  5852 net.cpp:434] label_data_1_split <- label
I1010 14:35:16.614001  5852 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1010 14:35:16.614007  5852 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1010 14:35:16.614140  5852 net.cpp:150] Setting up label_data_1_split
I1010 14:35:16.614145  5852 net.cpp:157] Top shape: 100 (100)
I1010 14:35:16.614164  5852 net.cpp:157] Top shape: 100 (100)
I1010 14:35:16.614166  5852 net.cpp:165] Memory required for data: 1230000
I1010 14:35:16.614168  5852 layer_factory.hpp:77] Creating layer conv1
I1010 14:35:16.614177  5852 net.cpp:100] Creating Layer conv1
I1010 14:35:16.614181  5852 net.cpp:434] conv1 <- data
I1010 14:35:16.614183  5852 net.cpp:408] conv1 -> conv1
I1010 14:35:16.615262  5852 net.cpp:150] Setting up conv1
I1010 14:35:16.615284  5852 net.cpp:157] Top shape: 100 96 30 30 (8640000)
I1010 14:35:16.615288  5852 net.cpp:165] Memory required for data: 35790000
I1010 14:35:16.615309  5852 layer_factory.hpp:77] Creating layer relu1
I1010 14:35:16.615315  5852 net.cpp:100] Creating Layer relu1
I1010 14:35:16.615317  5852 net.cpp:434] relu1 <- conv1
I1010 14:35:16.615321  5852 net.cpp:395] relu1 -> conv1 (in-place)
I1010 14:35:16.615505  5852 net.cpp:150] Setting up relu1
I1010 14:35:16.615514  5852 net.cpp:157] Top shape: 100 96 30 30 (8640000)
I1010 14:35:16.615530  5852 net.cpp:165] Memory required for data: 70350000
I1010 14:35:16.615532  5852 layer_factory.hpp:77] Creating layer conv2
I1010 14:35:16.615540  5852 net.cpp:100] Creating Layer conv2
I1010 14:35:16.615542  5852 net.cpp:434] conv2 <- conv1
I1010 14:35:16.615561  5852 net.cpp:408] conv2 -> conv2
I1010 14:35:16.617015  5852 net.cpp:150] Setting up conv2
I1010 14:35:16.617027  5852 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1010 14:35:16.617028  5852 net.cpp:165] Memory required for data: 100455600
I1010 14:35:16.617036  5852 layer_factory.hpp:77] Creating layer bn1
I1010 14:35:16.617040  5852 net.cpp:100] Creating Layer bn1
I1010 14:35:16.617043  5852 net.cpp:434] bn1 <- conv2
I1010 14:35:16.617046  5852 net.cpp:408] bn1 -> bn1
I1010 14:35:16.617239  5852 net.cpp:150] Setting up bn1
I1010 14:35:16.617246  5852 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1010 14:35:16.617249  5852 net.cpp:165] Memory required for data: 130561200
I1010 14:35:16.617255  5852 layer_factory.hpp:77] Creating layer relu2
I1010 14:35:16.617262  5852 net.cpp:100] Creating Layer relu2
I1010 14:35:16.617264  5852 net.cpp:434] relu2 <- bn1
I1010 14:35:16.617267  5852 net.cpp:395] relu2 -> bn1 (in-place)
I1010 14:35:16.617509  5852 net.cpp:150] Setting up relu2
I1010 14:35:16.617522  5852 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1010 14:35:16.617524  5852 net.cpp:165] Memory required for data: 160666800
I1010 14:35:16.617527  5852 layer_factory.hpp:77] Creating layer drop1
I1010 14:35:16.617532  5852 net.cpp:100] Creating Layer drop1
I1010 14:35:16.617533  5852 net.cpp:434] drop1 <- bn1
I1010 14:35:16.617538  5852 net.cpp:408] drop1 -> drop1
I1010 14:35:16.617576  5852 net.cpp:150] Setting up drop1
I1010 14:35:16.617581  5852 net.cpp:157] Top shape: 100 96 28 28 (7526400)
I1010 14:35:16.617583  5852 net.cpp:165] Memory required for data: 190772400
I1010 14:35:16.617585  5852 layer_factory.hpp:77] Creating layer conv3
I1010 14:35:16.617594  5852 net.cpp:100] Creating Layer conv3
I1010 14:35:16.617597  5852 net.cpp:434] conv3 <- drop1
I1010 14:35:16.617601  5852 net.cpp:408] conv3 -> conv3
I1010 14:35:16.618801  5852 net.cpp:150] Setting up conv3
I1010 14:35:16.618810  5852 net.cpp:157] Top shape: 100 96 13 13 (1622400)
I1010 14:35:16.618813  5852 net.cpp:165] Memory required for data: 197262000
I1010 14:35:16.618820  5852 layer_factory.hpp:77] Creating layer relu3
I1010 14:35:16.618825  5852 net.cpp:100] Creating Layer relu3
I1010 14:35:16.618829  5852 net.cpp:434] relu3 <- conv3
I1010 14:35:16.618832  5852 net.cpp:395] relu3 -> conv3 (in-place)
I1010 14:35:16.619063  5852 net.cpp:150] Setting up relu3
I1010 14:35:16.619073  5852 net.cpp:157] Top shape: 100 96 13 13 (1622400)
I1010 14:35:16.619076  5852 net.cpp:165] Memory required for data: 203751600
I1010 14:35:16.619078  5852 layer_factory.hpp:77] Creating layer conv4
I1010 14:35:16.619084  5852 net.cpp:100] Creating Layer conv4
I1010 14:35:16.619087  5852 net.cpp:434] conv4 <- conv3
I1010 14:35:16.619092  5852 net.cpp:408] conv4 -> conv4
I1010 14:35:16.621716  5852 net.cpp:150] Setting up conv4
I1010 14:35:16.621731  5852 net.cpp:157] Top shape: 100 192 11 11 (2323200)
I1010 14:35:16.621734  5852 net.cpp:165] Memory required for data: 213044400
I1010 14:35:16.621739  5852 layer_factory.hpp:77] Creating layer relu4
I1010 14:35:16.621745  5852 net.cpp:100] Creating Layer relu4
I1010 14:35:16.621748  5852 net.cpp:434] relu4 <- conv4
I1010 14:35:16.621752  5852 net.cpp:395] relu4 -> conv4 (in-place)
I1010 14:35:16.621960  5852 net.cpp:150] Setting up relu4
I1010 14:35:16.621966  5852 net.cpp:157] Top shape: 100 192 11 11 (2323200)
I1010 14:35:16.621968  5852 net.cpp:165] Memory required for data: 222337200
I1010 14:35:16.621971  5852 layer_factory.hpp:77] Creating layer conv5
I1010 14:35:16.621991  5852 net.cpp:100] Creating Layer conv5
I1010 14:35:16.621994  5852 net.cpp:434] conv5 <- conv4
I1010 14:35:16.621997  5852 net.cpp:408] conv5 -> conv5
I1010 14:35:16.624987  5852 net.cpp:150] Setting up conv5
I1010 14:35:16.625001  5852 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1010 14:35:16.625003  5852 net.cpp:165] Memory required for data: 228558000
I1010 14:35:16.625010  5852 layer_factory.hpp:77] Creating layer bn2
I1010 14:35:16.625013  5852 net.cpp:100] Creating Layer bn2
I1010 14:35:16.625015  5852 net.cpp:434] bn2 <- conv5
I1010 14:35:16.625020  5852 net.cpp:408] bn2 -> bn2
I1010 14:35:16.625233  5852 net.cpp:150] Setting up bn2
I1010 14:35:16.625238  5852 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1010 14:35:16.625241  5852 net.cpp:165] Memory required for data: 234778800
I1010 14:35:16.625246  5852 layer_factory.hpp:77] Creating layer relu5
I1010 14:35:16.625250  5852 net.cpp:100] Creating Layer relu5
I1010 14:35:16.625252  5852 net.cpp:434] relu5 <- bn2
I1010 14:35:16.625255  5852 net.cpp:395] relu5 -> bn2 (in-place)
I1010 14:35:16.625542  5852 net.cpp:150] Setting up relu5
I1010 14:35:16.625551  5852 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1010 14:35:16.625553  5852 net.cpp:165] Memory required for data: 240999600
I1010 14:35:16.625555  5852 layer_factory.hpp:77] Creating layer drop4
I1010 14:35:16.625560  5852 net.cpp:100] Creating Layer drop4
I1010 14:35:16.625561  5852 net.cpp:434] drop4 <- bn2
I1010 14:35:16.625566  5852 net.cpp:408] drop4 -> drop4
I1010 14:35:16.625614  5852 net.cpp:150] Setting up drop4
I1010 14:35:16.625633  5852 net.cpp:157] Top shape: 100 192 9 9 (1555200)
I1010 14:35:16.625635  5852 net.cpp:165] Memory required for data: 247220400
I1010 14:35:16.625638  5852 layer_factory.hpp:77] Creating layer conv6
I1010 14:35:16.625659  5852 net.cpp:100] Creating Layer conv6
I1010 14:35:16.625661  5852 net.cpp:434] conv6 <- drop4
I1010 14:35:16.625679  5852 net.cpp:408] conv6 -> conv6
I1010 14:35:16.628348  5852 net.cpp:150] Setting up conv6
I1010 14:35:16.628358  5852 net.cpp:157] Top shape: 100 192 4 4 (307200)
I1010 14:35:16.628360  5852 net.cpp:165] Memory required for data: 248449200
I1010 14:35:16.628368  5852 layer_factory.hpp:77] Creating layer relu6
I1010 14:35:16.628374  5852 net.cpp:100] Creating Layer relu6
I1010 14:35:16.628376  5852 net.cpp:434] relu6 <- conv6
I1010 14:35:16.628379  5852 net.cpp:395] relu6 -> conv6 (in-place)
I1010 14:35:16.628648  5852 net.cpp:150] Setting up relu6
I1010 14:35:16.628656  5852 net.cpp:157] Top shape: 100 192 4 4 (307200)
I1010 14:35:16.628659  5852 net.cpp:165] Memory required for data: 249678000
I1010 14:35:16.628662  5852 layer_factory.hpp:77] Creating layer conv7
I1010 14:35:16.628669  5852 net.cpp:100] Creating Layer conv7
I1010 14:35:16.628685  5852 net.cpp:434] conv7 <- conv6
I1010 14:35:16.628690  5852 net.cpp:408] conv7 -> conv7
I1010 14:35:16.631453  5852 net.cpp:150] Setting up conv7
I1010 14:35:16.631464  5852 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1010 14:35:16.631466  5852 net.cpp:165] Memory required for data: 249985200
I1010 14:35:16.631471  5852 layer_factory.hpp:77] Creating layer relu7
I1010 14:35:16.631474  5852 net.cpp:100] Creating Layer relu7
I1010 14:35:16.631477  5852 net.cpp:434] relu7 <- conv7
I1010 14:35:16.631480  5852 net.cpp:395] relu7 -> conv7 (in-place)
I1010 14:35:16.631654  5852 net.cpp:150] Setting up relu7
I1010 14:35:16.631660  5852 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1010 14:35:16.631662  5852 net.cpp:165] Memory required for data: 250292400
I1010 14:35:16.631664  5852 layer_factory.hpp:77] Creating layer conv8
I1010 14:35:16.631672  5852 net.cpp:100] Creating Layer conv8
I1010 14:35:16.631675  5852 net.cpp:434] conv8 <- conv7
I1010 14:35:16.631677  5852 net.cpp:408] conv8 -> conv8
I1010 14:35:16.632688  5852 net.cpp:150] Setting up conv8
I1010 14:35:16.632696  5852 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1010 14:35:16.632699  5852 net.cpp:165] Memory required for data: 250599600
I1010 14:35:16.632702  5852 layer_factory.hpp:77] Creating layer relu8
I1010 14:35:16.632707  5852 net.cpp:100] Creating Layer relu8
I1010 14:35:16.632709  5852 net.cpp:434] relu8 <- conv8
I1010 14:35:16.632712  5852 net.cpp:395] relu8 -> conv8 (in-place)
I1010 14:35:16.632995  5852 net.cpp:150] Setting up relu8
I1010 14:35:16.633004  5852 net.cpp:157] Top shape: 100 192 2 2 (76800)
I1010 14:35:16.633007  5852 net.cpp:165] Memory required for data: 250906800
I1010 14:35:16.633008  5852 layer_factory.hpp:77] Creating layer conv9
I1010 14:35:16.633014  5852 net.cpp:100] Creating Layer conv9
I1010 14:35:16.633016  5852 net.cpp:434] conv9 <- conv8
I1010 14:35:16.633020  5852 net.cpp:408] conv9 -> conv9
I1010 14:35:16.633790  5852 net.cpp:150] Setting up conv9
I1010 14:35:16.633800  5852 net.cpp:157] Top shape: 100 10 2 2 (4000)
I1010 14:35:16.633803  5852 net.cpp:165] Memory required for data: 250922800
I1010 14:35:16.633806  5852 layer_factory.hpp:77] Creating layer relu9
I1010 14:35:16.633810  5852 net.cpp:100] Creating Layer relu9
I1010 14:35:16.633812  5852 net.cpp:434] relu9 <- conv9
I1010 14:35:16.633824  5852 net.cpp:395] relu9 -> conv9 (in-place)
I1010 14:35:16.634088  5852 net.cpp:150] Setting up relu9
I1010 14:35:16.634097  5852 net.cpp:157] Top shape: 100 10 2 2 (4000)
I1010 14:35:16.634099  5852 net.cpp:165] Memory required for data: 250938800
I1010 14:35:16.634101  5852 layer_factory.hpp:77] Creating layer pool
I1010 14:35:16.634107  5852 net.cpp:100] Creating Layer pool
I1010 14:35:16.634109  5852 net.cpp:434] pool <- conv9
I1010 14:35:16.634114  5852 net.cpp:408] pool -> pool
I1010 14:35:16.634301  5852 net.cpp:150] Setting up pool
I1010 14:35:16.634307  5852 net.cpp:157] Top shape: 100 10 1 1 (1000)
I1010 14:35:16.634310  5852 net.cpp:165] Memory required for data: 250942800
I1010 14:35:16.634311  5852 layer_factory.hpp:77] Creating layer flatten
I1010 14:35:16.634315  5852 net.cpp:100] Creating Layer flatten
I1010 14:35:16.634317  5852 net.cpp:434] flatten <- pool
I1010 14:35:16.634320  5852 net.cpp:408] flatten -> flatten
I1010 14:35:16.634356  5852 net.cpp:150] Setting up flatten
I1010 14:35:16.634361  5852 net.cpp:157] Top shape: 100 10 (1000)
I1010 14:35:16.634363  5852 net.cpp:165] Memory required for data: 250946800
I1010 14:35:16.634366  5852 layer_factory.hpp:77] Creating layer score
I1010 14:35:16.634369  5852 net.cpp:100] Creating Layer score
I1010 14:35:16.634371  5852 net.cpp:434] score <- flatten
I1010 14:35:16.634389  5852 net.cpp:408] score -> score
I1010 14:35:16.634536  5852 net.cpp:150] Setting up score
I1010 14:35:16.634541  5852 net.cpp:157] Top shape: 100 10 (1000)
I1010 14:35:16.634542  5852 net.cpp:165] Memory required for data: 250950800
I1010 14:35:16.634546  5852 layer_factory.hpp:77] Creating layer score_score_0_split
I1010 14:35:16.634549  5852 net.cpp:100] Creating Layer score_score_0_split
I1010 14:35:16.634552  5852 net.cpp:434] score_score_0_split <- score
I1010 14:35:16.634555  5852 net.cpp:408] score_score_0_split -> score_score_0_split_0
I1010 14:35:16.634559  5852 net.cpp:408] score_score_0_split -> score_score_0_split_1
I1010 14:35:16.634601  5852 net.cpp:150] Setting up score_score_0_split
I1010 14:35:16.634619  5852 net.cpp:157] Top shape: 100 10 (1000)
I1010 14:35:16.634623  5852 net.cpp:157] Top shape: 100 10 (1000)
I1010 14:35:16.634624  5852 net.cpp:165] Memory required for data: 250958800
I1010 14:35:16.634625  5852 layer_factory.hpp:77] Creating layer accuracy
I1010 14:35:16.634642  5852 net.cpp:100] Creating Layer accuracy
I1010 14:35:16.634645  5852 net.cpp:434] accuracy <- score_score_0_split_0
I1010 14:35:16.634647  5852 net.cpp:434] accuracy <- label_data_1_split_0
I1010 14:35:16.634665  5852 net.cpp:408] accuracy -> accuracy
I1010 14:35:16.634671  5852 net.cpp:150] Setting up accuracy
I1010 14:35:16.634673  5852 net.cpp:157] Top shape: (1)
I1010 14:35:16.634675  5852 net.cpp:165] Memory required for data: 250958804
I1010 14:35:16.634677  5852 layer_factory.hpp:77] Creating layer loss
I1010 14:35:16.634680  5852 net.cpp:100] Creating Layer loss
I1010 14:35:16.634685  5852 net.cpp:434] loss <- score_score_0_split_1
I1010 14:35:16.634699  5852 net.cpp:434] loss <- label_data_1_split_1
I1010 14:35:16.634702  5852 net.cpp:408] loss -> loss
I1010 14:35:16.634722  5852 layer_factory.hpp:77] Creating layer loss
I1010 14:35:16.635061  5852 net.cpp:150] Setting up loss
I1010 14:35:16.635069  5852 net.cpp:157] Top shape: (1)
I1010 14:35:16.635085  5852 net.cpp:160]     with loss weight 1
I1010 14:35:16.635093  5852 net.cpp:165] Memory required for data: 250958808
I1010 14:35:16.635095  5852 net.cpp:226] loss needs backward computation.
I1010 14:35:16.635098  5852 net.cpp:228] accuracy does not need backward computation.
I1010 14:35:16.635102  5852 net.cpp:226] score_score_0_split needs backward computation.
I1010 14:35:16.635104  5852 net.cpp:226] score needs backward computation.
I1010 14:35:16.635120  5852 net.cpp:226] flatten needs backward computation.
I1010 14:35:16.635123  5852 net.cpp:226] pool needs backward computation.
I1010 14:35:16.635124  5852 net.cpp:226] relu9 needs backward computation.
I1010 14:35:16.635126  5852 net.cpp:226] conv9 needs backward computation.
I1010 14:35:16.635135  5852 net.cpp:226] relu8 needs backward computation.
I1010 14:35:16.635138  5852 net.cpp:226] conv8 needs backward computation.
I1010 14:35:16.635139  5852 net.cpp:226] relu7 needs backward computation.
I1010 14:35:16.635141  5852 net.cpp:226] conv7 needs backward computation.
I1010 14:35:16.635143  5852 net.cpp:226] relu6 needs backward computation.
I1010 14:35:16.635145  5852 net.cpp:226] conv6 needs backward computation.
I1010 14:35:16.635148  5852 net.cpp:226] drop4 needs backward computation.
I1010 14:35:16.635149  5852 net.cpp:226] relu5 needs backward computation.
I1010 14:35:16.635151  5852 net.cpp:226] bn2 needs backward computation.
I1010 14:35:16.635154  5852 net.cpp:226] conv5 needs backward computation.
I1010 14:35:16.635155  5852 net.cpp:226] relu4 needs backward computation.
I1010 14:35:16.635157  5852 net.cpp:226] conv4 needs backward computation.
I1010 14:35:16.635160  5852 net.cpp:226] relu3 needs backward computation.
I1010 14:35:16.635162  5852 net.cpp:226] conv3 needs backward computation.
I1010 14:35:16.635164  5852 net.cpp:226] drop1 needs backward computation.
I1010 14:35:16.635166  5852 net.cpp:226] relu2 needs backward computation.
I1010 14:35:16.635169  5852 net.cpp:226] bn1 needs backward computation.
I1010 14:35:16.635170  5852 net.cpp:226] conv2 needs backward computation.
I1010 14:35:16.635185  5852 net.cpp:226] relu1 needs backward computation.
I1010 14:35:16.635187  5852 net.cpp:226] conv1 needs backward computation.
I1010 14:35:16.635190  5852 net.cpp:228] label_data_1_split does not need backward computation.
I1010 14:35:16.635193  5852 net.cpp:228] data does not need backward computation.
I1010 14:35:16.635195  5852 net.cpp:270] This network produces output accuracy
I1010 14:35:16.635197  5852 net.cpp:270] This network produces output loss
I1010 14:35:16.635213  5852 net.cpp:283] Network initialization done.
I1010 14:35:16.635293  5852 solver.cpp:60] Solver scaffolding done.
I1010 14:35:16.636139  5852 caffe.cpp:251] Starting Optimization
I1010 14:35:16.636143  5852 solver.cpp:279] Solving 
I1010 14:35:16.636145  5852 solver.cpp:280] Learning Rate Policy: step
I1010 14:35:16.637117  5852 solver.cpp:337] Iteration 0, Testing net (#0)
I1010 14:35:20.152981  5852 solver.cpp:404]     Test net output #0: accuracy = 0.1009
I1010 14:35:20.153003  5852 solver.cpp:404]     Test net output #1: loss = 78.5243 (* 1 = 78.5243 loss)
I1010 14:35:20.194072  5852 solver.cpp:228] Iteration 0, loss = 2.50528
I1010 14:35:20.194093  5852 solver.cpp:244]     Train net output #0: accuracy = 0.0625
I1010 14:35:20.194100  5852 solver.cpp:244]     Train net output #1: loss = 2.50528 (* 1 = 2.50528 loss)
I1010 14:35:20.194108  5852 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I1010 14:35:24.353808  5852 solver.cpp:228] Iteration 50, loss = 2.29904
I1010 14:35:24.353842  5852 solver.cpp:244]     Train net output #0: accuracy = 0.046875
I1010 14:35:24.353848  5852 solver.cpp:244]     Train net output #1: loss = 2.29904 (* 1 = 2.29904 loss)
I1010 14:35:24.353852  5852 sgd_solver.cpp:106] Iteration 50, lr = 0.0001
I1010 14:35:28.516228  5852 solver.cpp:228] Iteration 100, loss = 2.27449
I1010 14:35:28.516248  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:35:28.516254  5852 solver.cpp:244]     Train net output #1: loss = 2.27449 (* 1 = 2.27449 loss)
I1010 14:35:28.516258  5852 sgd_solver.cpp:106] Iteration 100, lr = 0.0001
I1010 14:35:32.674284  5852 solver.cpp:228] Iteration 150, loss = 2.24528
I1010 14:35:32.674304  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:35:32.674311  5852 solver.cpp:244]     Train net output #1: loss = 2.24528 (* 1 = 2.24528 loss)
I1010 14:35:32.674315  5852 sgd_solver.cpp:106] Iteration 150, lr = 0.0001
I1010 14:35:36.837206  5852 solver.cpp:228] Iteration 200, loss = 2.25477
I1010 14:35:36.837226  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:35:36.837234  5852 solver.cpp:244]     Train net output #1: loss = 2.25477 (* 1 = 2.25477 loss)
I1010 14:35:36.837236  5852 sgd_solver.cpp:106] Iteration 200, lr = 0.0001
I1010 14:35:40.973292  5852 solver.cpp:337] Iteration 250, Testing net (#0)
I1010 14:35:44.484541  5852 solver.cpp:404]     Test net output #0: accuracy = 0.1425
I1010 14:35:44.484565  5852 solver.cpp:404]     Test net output #1: loss = 2.2328 (* 1 = 2.2328 loss)
I1010 14:35:44.514333  5852 solver.cpp:228] Iteration 250, loss = 2.22095
I1010 14:35:44.514353  5852 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:35:44.514359  5852 solver.cpp:244]     Train net output #1: loss = 2.22095 (* 1 = 2.22095 loss)
I1010 14:35:44.514364  5852 sgd_solver.cpp:106] Iteration 250, lr = 0.0001
I1010 14:35:48.674160  5852 solver.cpp:228] Iteration 300, loss = 2.21843
I1010 14:35:48.674214  5852 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:35:48.674222  5852 solver.cpp:244]     Train net output #1: loss = 2.21843 (* 1 = 2.21843 loss)
I1010 14:35:48.674238  5852 sgd_solver.cpp:106] Iteration 300, lr = 0.0001
I1010 14:35:52.833497  5852 solver.cpp:228] Iteration 350, loss = 2.28331
I1010 14:35:52.833518  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:35:52.833524  5852 solver.cpp:244]     Train net output #1: loss = 2.28331 (* 1 = 2.28331 loss)
I1010 14:35:52.833528  5852 sgd_solver.cpp:106] Iteration 350, lr = 0.0001
I1010 14:35:56.992167  5852 solver.cpp:228] Iteration 400, loss = 2.2827
I1010 14:35:56.992202  5852 solver.cpp:244]     Train net output #0: accuracy = 0.0625
I1010 14:35:56.992208  5852 solver.cpp:244]     Train net output #1: loss = 2.2827 (* 1 = 2.2827 loss)
I1010 14:35:56.992211  5852 sgd_solver.cpp:106] Iteration 400, lr = 0.0001
I1010 14:36:01.151546  5852 solver.cpp:228] Iteration 450, loss = 2.24335
I1010 14:36:01.151579  5852 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I1010 14:36:01.151587  5852 solver.cpp:244]     Train net output #1: loss = 2.24335 (* 1 = 2.24335 loss)
I1010 14:36:01.151590  5852 sgd_solver.cpp:106] Iteration 450, lr = 0.0001
I1010 14:36:05.289664  5852 solver.cpp:337] Iteration 500, Testing net (#0)
I1010 14:36:08.802865  5852 solver.cpp:404]     Test net output #0: accuracy = 0.1554
I1010 14:36:08.802902  5852 solver.cpp:404]     Test net output #1: loss = 2.19407 (* 1 = 2.19407 loss)
I1010 14:36:08.832690  5852 solver.cpp:228] Iteration 500, loss = 2.18064
I1010 14:36:08.832712  5852 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I1010 14:36:08.832720  5852 solver.cpp:244]     Train net output #1: loss = 2.18064 (* 1 = 2.18064 loss)
I1010 14:36:08.832723  5852 sgd_solver.cpp:106] Iteration 500, lr = 0.0001
I1010 14:36:12.989773  5852 solver.cpp:228] Iteration 550, loss = 2.15927
I1010 14:36:12.989795  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:36:12.989802  5852 solver.cpp:244]     Train net output #1: loss = 2.15927 (* 1 = 2.15927 loss)
I1010 14:36:12.989806  5852 sgd_solver.cpp:106] Iteration 550, lr = 0.0001
I1010 14:36:17.151391  5852 solver.cpp:228] Iteration 600, loss = 2.25732
I1010 14:36:17.151412  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:36:17.151419  5852 solver.cpp:244]     Train net output #1: loss = 2.25732 (* 1 = 2.25732 loss)
I1010 14:36:17.151422  5852 sgd_solver.cpp:106] Iteration 600, lr = 0.0001
I1010 14:36:21.312706  5852 solver.cpp:228] Iteration 650, loss = 2.10506
I1010 14:36:21.312750  5852 solver.cpp:244]     Train net output #0: accuracy = 0.25
I1010 14:36:21.312758  5852 solver.cpp:244]     Train net output #1: loss = 2.10506 (* 1 = 2.10506 loss)
I1010 14:36:21.312762  5852 sgd_solver.cpp:106] Iteration 650, lr = 0.0001
I1010 14:36:25.471632  5852 solver.cpp:228] Iteration 700, loss = 2.13948
I1010 14:36:25.471650  5852 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I1010 14:36:25.471673  5852 solver.cpp:244]     Train net output #1: loss = 2.13948 (* 1 = 2.13948 loss)
I1010 14:36:25.471675  5852 sgd_solver.cpp:106] Iteration 700, lr = 0.0001
I1010 14:36:29.605579  5852 solver.cpp:337] Iteration 750, Testing net (#0)
I1010 14:36:33.115324  5852 solver.cpp:404]     Test net output #0: accuracy = 0.1526
I1010 14:36:33.115361  5852 solver.cpp:404]     Test net output #1: loss = 2.17231 (* 1 = 2.17231 loss)
I1010 14:36:33.145166  5852 solver.cpp:228] Iteration 750, loss = 2.27013
I1010 14:36:33.145186  5852 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:36:33.145193  5852 solver.cpp:244]     Train net output #1: loss = 2.27013 (* 1 = 2.27013 loss)
I1010 14:36:33.145197  5852 sgd_solver.cpp:106] Iteration 750, lr = 0.0001
I1010 14:36:37.307118  5852 solver.cpp:228] Iteration 800, loss = 2.17804
I1010 14:36:37.307138  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:36:37.307145  5852 solver.cpp:244]     Train net output #1: loss = 2.17804 (* 1 = 2.17804 loss)
I1010 14:36:37.307163  5852 sgd_solver.cpp:106] Iteration 800, lr = 0.0001
I1010 14:36:41.487392  5852 solver.cpp:228] Iteration 850, loss = 2.13292
I1010 14:36:41.487412  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:36:41.487419  5852 solver.cpp:244]     Train net output #1: loss = 2.13292 (* 1 = 2.13292 loss)
I1010 14:36:41.487423  5852 sgd_solver.cpp:106] Iteration 850, lr = 0.0001
I1010 14:36:45.690448  5852 solver.cpp:228] Iteration 900, loss = 2.18789
I1010 14:36:45.690469  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:36:45.690475  5852 solver.cpp:244]     Train net output #1: loss = 2.18789 (* 1 = 2.18789 loss)
I1010 14:36:45.690479  5852 sgd_solver.cpp:106] Iteration 900, lr = 0.0001
I1010 14:36:49.882334  5852 solver.cpp:228] Iteration 950, loss = 2.20094
I1010 14:36:49.882369  5852 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:36:49.882375  5852 solver.cpp:244]     Train net output #1: loss = 2.20094 (* 1 = 2.20094 loss)
I1010 14:36:49.882378  5852 sgd_solver.cpp:106] Iteration 950, lr = 0.0001
I1010 14:36:54.019753  5852 solver.cpp:337] Iteration 1000, Testing net (#0)
I1010 14:36:57.525241  5852 solver.cpp:404]     Test net output #0: accuracy = 0.1532
I1010 14:36:57.525264  5852 solver.cpp:404]     Test net output #1: loss = 2.16071 (* 1 = 2.16071 loss)
I1010 14:36:57.551672  5852 solver.cpp:228] Iteration 1000, loss = 2.14125
I1010 14:36:57.551687  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:36:57.551707  5852 solver.cpp:244]     Train net output #1: loss = 2.14125 (* 1 = 2.14125 loss)
I1010 14:36:57.551712  5852 sgd_solver.cpp:106] Iteration 1000, lr = 0.0001
I1010 14:37:01.732122  5852 solver.cpp:228] Iteration 1050, loss = 2.209
I1010 14:37:01.732142  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:37:01.732148  5852 solver.cpp:244]     Train net output #1: loss = 2.209 (* 1 = 2.209 loss)
I1010 14:37:01.732152  5852 sgd_solver.cpp:106] Iteration 1050, lr = 0.0001
I1010 14:37:05.913435  5852 solver.cpp:228] Iteration 1100, loss = 2.20868
I1010 14:37:05.913456  5852 solver.cpp:244]     Train net output #0: accuracy = 0.125
I1010 14:37:05.913463  5852 solver.cpp:244]     Train net output #1: loss = 2.20868 (* 1 = 2.20868 loss)
I1010 14:37:05.913467  5852 sgd_solver.cpp:106] Iteration 1100, lr = 0.0001
I1010 14:37:10.099023  5852 solver.cpp:228] Iteration 1150, loss = 2.1662
I1010 14:37:10.099058  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:37:10.099066  5852 solver.cpp:244]     Train net output #1: loss = 2.1662 (* 1 = 2.1662 loss)
I1010 14:37:10.099069  5852 sgd_solver.cpp:106] Iteration 1150, lr = 0.0001
I1010 14:37:14.282982  5852 solver.cpp:228] Iteration 1200, loss = 2.13529
I1010 14:37:14.283004  5852 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:37:14.283010  5852 solver.cpp:244]     Train net output #1: loss = 2.13529 (* 1 = 2.13529 loss)
I1010 14:37:14.283013  5852 sgd_solver.cpp:106] Iteration 1200, lr = 0.0001
I1010 14:37:18.419383  5852 solver.cpp:337] Iteration 1250, Testing net (#0)
I1010 14:37:21.931865  5852 solver.cpp:404]     Test net output #0: accuracy = 0.157
I1010 14:37:21.931890  5852 solver.cpp:404]     Test net output #1: loss = 2.15039 (* 1 = 2.15039 loss)
I1010 14:37:21.961833  5852 solver.cpp:228] Iteration 1250, loss = 2.15767
I1010 14:37:21.961851  5852 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I1010 14:37:21.961858  5852 solver.cpp:244]     Train net output #1: loss = 2.15767 (* 1 = 2.15767 loss)
I1010 14:37:21.961861  5852 sgd_solver.cpp:106] Iteration 1250, lr = 0.0001
I1010 14:37:26.115707  5852 solver.cpp:228] Iteration 1300, loss = 2.15684
I1010 14:37:26.115802  5852 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:37:26.115824  5852 solver.cpp:244]     Train net output #1: loss = 2.15684 (* 1 = 2.15684 loss)
I1010 14:37:26.115828  5852 sgd_solver.cpp:106] Iteration 1300, lr = 0.0001
I1010 14:37:30.277431  5852 solver.cpp:228] Iteration 1350, loss = 2.07303
I1010 14:37:30.277453  5852 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I1010 14:37:30.277461  5852 solver.cpp:244]     Train net output #1: loss = 2.07303 (* 1 = 2.07303 loss)
I1010 14:37:30.277463  5852 sgd_solver.cpp:106] Iteration 1350, lr = 0.0001
I1010 14:37:34.436424  5852 solver.cpp:228] Iteration 1400, loss = 2.19805
I1010 14:37:34.436460  5852 solver.cpp:244]     Train net output #0: accuracy = 0.078125
I1010 14:37:34.436467  5852 solver.cpp:244]     Train net output #1: loss = 2.19805 (* 1 = 2.19805 loss)
I1010 14:37:34.436471  5852 sgd_solver.cpp:106] Iteration 1400, lr = 0.0001
I1010 14:37:38.593508  5852 solver.cpp:228] Iteration 1450, loss = 2.15823
I1010 14:37:38.593541  5852 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I1010 14:37:38.593549  5852 solver.cpp:244]     Train net output #1: loss = 2.15823 (* 1 = 2.15823 loss)
I1010 14:37:38.593552  5852 sgd_solver.cpp:106] Iteration 1450, lr = 0.0001
I1010 14:37:42.725888  5852 solver.cpp:337] Iteration 1500, Testing net (#0)
I1010 14:37:46.241575  5852 solver.cpp:404]     Test net output #0: accuracy = 0.165
I1010 14:37:46.241611  5852 solver.cpp:404]     Test net output #1: loss = 2.14292 (* 1 = 2.14292 loss)
I1010 14:37:46.271497  5852 solver.cpp:228] Iteration 1500, loss = 2.16429
I1010 14:37:46.271517  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:37:46.271524  5852 solver.cpp:244]     Train net output #1: loss = 2.16429 (* 1 = 2.16429 loss)
I1010 14:37:46.271528  5852 sgd_solver.cpp:106] Iteration 1500, lr = 0.0001
I1010 14:37:50.431593  5852 solver.cpp:228] Iteration 1550, loss = 2.11125
I1010 14:37:50.431613  5852 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I1010 14:37:50.431620  5852 solver.cpp:244]     Train net output #1: loss = 2.11125 (* 1 = 2.11125 loss)
I1010 14:37:50.431638  5852 sgd_solver.cpp:106] Iteration 1550, lr = 0.0001
I1010 14:37:54.593413  5852 solver.cpp:228] Iteration 1600, loss = 2.1967
I1010 14:37:54.593433  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:37:54.593441  5852 solver.cpp:244]     Train net output #1: loss = 2.1967 (* 1 = 2.1967 loss)
I1010 14:37:54.593458  5852 sgd_solver.cpp:106] Iteration 1600, lr = 0.0001
I1010 14:37:58.757401  5852 solver.cpp:228] Iteration 1650, loss = 2.20878
I1010 14:37:58.757469  5852 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:37:58.757477  5852 solver.cpp:244]     Train net output #1: loss = 2.20878 (* 1 = 2.20878 loss)
I1010 14:37:58.757494  5852 sgd_solver.cpp:106] Iteration 1650, lr = 0.0001
I1010 14:38:02.919191  5852 solver.cpp:228] Iteration 1700, loss = 2.21121
I1010 14:38:02.919211  5852 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I1010 14:38:02.919219  5852 solver.cpp:244]     Train net output #1: loss = 2.21121 (* 1 = 2.21121 loss)
I1010 14:38:02.919221  5852 sgd_solver.cpp:106] Iteration 1700, lr = 0.0001
I1010 14:38:07.056311  5852 solver.cpp:337] Iteration 1750, Testing net (#0)
I1010 14:38:10.568650  5852 solver.cpp:404]     Test net output #0: accuracy = 0.1725
I1010 14:38:10.568670  5852 solver.cpp:404]     Test net output #1: loss = 2.14043 (* 1 = 2.14043 loss)
I1010 14:38:10.598584  5852 solver.cpp:228] Iteration 1750, loss = 2.12693
I1010 14:38:10.598605  5852 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I1010 14:38:10.598613  5852 solver.cpp:244]     Train net output #1: loss = 2.12693 (* 1 = 2.12693 loss)
I1010 14:38:10.598616  5852 sgd_solver.cpp:106] Iteration 1750, lr = 0.0001
I1010 14:38:14.758714  5852 solver.cpp:228] Iteration 1800, loss = 2.10499
I1010 14:38:14.758733  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:38:14.758740  5852 solver.cpp:244]     Train net output #1: loss = 2.10499 (* 1 = 2.10499 loss)
I1010 14:38:14.758744  5852 sgd_solver.cpp:106] Iteration 1800, lr = 0.0001
I1010 14:38:18.921203  5852 solver.cpp:228] Iteration 1850, loss = 2.09563
I1010 14:38:18.921223  5852 solver.cpp:244]     Train net output #0: accuracy = 0.09375
I1010 14:38:18.921243  5852 solver.cpp:244]     Train net output #1: loss = 2.09563 (* 1 = 2.09563 loss)
I1010 14:38:18.921260  5852 sgd_solver.cpp:106] Iteration 1850, lr = 0.0001
I1010 14:38:23.085206  5852 solver.cpp:228] Iteration 1900, loss = 2.08735
I1010 14:38:23.085227  5852 solver.cpp:244]     Train net output #0: accuracy = 0.21875
I1010 14:38:23.085233  5852 solver.cpp:244]     Train net output #1: loss = 2.08735 (* 1 = 2.08735 loss)
I1010 14:38:23.085250  5852 sgd_solver.cpp:106] Iteration 1900, lr = 0.0001
I1010 14:38:27.253646  5852 solver.cpp:228] Iteration 1950, loss = 2.14737
I1010 14:38:27.253665  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:38:27.253672  5852 solver.cpp:244]     Train net output #1: loss = 2.14737 (* 1 = 2.14737 loss)
I1010 14:38:27.253690  5852 sgd_solver.cpp:106] Iteration 1950, lr = 0.0001
I1010 14:38:31.386538  5852 solver.cpp:337] Iteration 2000, Testing net (#0)
I1010 14:38:34.900601  5852 solver.cpp:404]     Test net output #0: accuracy = 0.1767
I1010 14:38:34.900625  5852 solver.cpp:404]     Test net output #1: loss = 2.13463 (* 1 = 2.13463 loss)
I1010 14:38:34.930470  5852 solver.cpp:228] Iteration 2000, loss = 2.14174
I1010 14:38:34.930491  5852 solver.cpp:244]     Train net output #0: accuracy = 0.21875
I1010 14:38:34.930497  5852 solver.cpp:244]     Train net output #1: loss = 2.14174 (* 1 = 2.14174 loss)
I1010 14:38:34.930502  5852 sgd_solver.cpp:106] Iteration 2000, lr = 0.0001
I1010 14:38:39.108289  5852 solver.cpp:228] Iteration 2050, loss = 2.15084
I1010 14:38:39.108325  5852 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I1010 14:38:39.108332  5852 solver.cpp:244]     Train net output #1: loss = 2.15084 (* 1 = 2.15084 loss)
I1010 14:38:39.108336  5852 sgd_solver.cpp:106] Iteration 2050, lr = 0.0001
I1010 14:38:43.322057  5852 solver.cpp:228] Iteration 2100, loss = 2.15208
I1010 14:38:43.322078  5852 solver.cpp:244]     Train net output #0: accuracy = 0.140625
I1010 14:38:43.322085  5852 solver.cpp:244]     Train net output #1: loss = 2.15208 (* 1 = 2.15208 loss)
I1010 14:38:43.322103  5852 sgd_solver.cpp:106] Iteration 2100, lr = 0.0001
I1010 14:38:47.554416  5852 solver.cpp:228] Iteration 2150, loss = 2.02264
I1010 14:38:47.554436  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:38:47.554443  5852 solver.cpp:244]     Train net output #1: loss = 2.02264 (* 1 = 2.02264 loss)
I1010 14:38:47.554447  5852 sgd_solver.cpp:106] Iteration 2150, lr = 0.0001
I1010 14:38:51.840713  5852 solver.cpp:228] Iteration 2200, loss = 2.06292
I1010 14:38:51.840734  5852 solver.cpp:244]     Train net output #0: accuracy = 0.265625
I1010 14:38:51.840740  5852 solver.cpp:244]     Train net output #1: loss = 2.06292 (* 1 = 2.06292 loss)
I1010 14:38:51.840744  5852 sgd_solver.cpp:106] Iteration 2200, lr = 0.0001
I1010 14:38:55.991961  5852 solver.cpp:337] Iteration 2250, Testing net (#0)
I1010 14:38:59.508412  5852 solver.cpp:404]     Test net output #0: accuracy = 0.181
I1010 14:38:59.508435  5852 solver.cpp:404]     Test net output #1: loss = 2.12489 (* 1 = 2.12489 loss)
I1010 14:38:59.538413  5852 solver.cpp:228] Iteration 2250, loss = 2.08904
I1010 14:38:59.538460  5852 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I1010 14:38:59.538481  5852 solver.cpp:244]     Train net output #1: loss = 2.08904 (* 1 = 2.08904 loss)
I1010 14:38:59.538486  5852 sgd_solver.cpp:106] Iteration 2250, lr = 0.0001
I1010 14:39:03.698776  5852 solver.cpp:228] Iteration 2300, loss = 2.14549
I1010 14:39:03.698874  5852 solver.cpp:244]     Train net output #0: accuracy = 0.109375
I1010 14:39:03.698884  5852 solver.cpp:244]     Train net output #1: loss = 2.14549 (* 1 = 2.14549 loss)
I1010 14:39:03.698889  5852 sgd_solver.cpp:106] Iteration 2300, lr = 0.0001
I1010 14:39:07.859277  5852 solver.cpp:228] Iteration 2350, loss = 2.02666
I1010 14:39:07.859304  5852 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I1010 14:39:07.859311  5852 solver.cpp:244]     Train net output #1: loss = 2.02666 (* 1 = 2.02666 loss)
I1010 14:39:07.859315  5852 sgd_solver.cpp:106] Iteration 2350, lr = 0.0001
I1010 14:39:12.019079  5852 solver.cpp:228] Iteration 2400, loss = 2.12571
I1010 14:39:12.019112  5852 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I1010 14:39:12.019119  5852 solver.cpp:244]     Train net output #1: loss = 2.12571 (* 1 = 2.12571 loss)
I1010 14:39:12.019122  5852 sgd_solver.cpp:106] Iteration 2400, lr = 0.0001
I1010 14:39:16.176100  5852 solver.cpp:228] Iteration 2450, loss = 2.03364
I1010 14:39:16.176118  5852 solver.cpp:244]     Train net output #0: accuracy = 0.25
I1010 14:39:16.176126  5852 solver.cpp:244]     Train net output #1: loss = 2.03364 (* 1 = 2.03364 loss)
I1010 14:39:16.176128  5852 sgd_solver.cpp:106] Iteration 2450, lr = 0.0001
I1010 14:39:20.308115  5852 solver.cpp:454] Snapshotting to binary proto file cifar-10_relu_AdaGrad_iter_2500.caffemodel
I1010 14:39:20.324440  5852 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar-10_relu_AdaGrad_iter_2500.solverstate
I1010 14:39:20.360543  5852 solver.cpp:317] Iteration 2500, loss = 2.00526
I1010 14:39:20.360564  5852 solver.cpp:337] Iteration 2500, Testing net (#0)
I1010 14:39:23.871479  5852 solver.cpp:404]     Test net output #0: accuracy = 0.1826
I1010 14:39:23.871501  5852 solver.cpp:404]     Test net output #1: loss = 2.12129 (* 1 = 2.12129 loss)
I1010 14:39:23.871505  5852 solver.cpp:322] Optimization Done.
I1010 14:39:23.871507  5852 caffe.cpp:254] Optimization Done.
